{
  "skills": [
    {
      "id": "aws-advisor",
      "name": "aws-advisor",
      "description": "Expert AWS Cloud Advisor for architecture design, security review, and implementation guidance. Leverages AWS MCP tools for accurate, documentation-backed answers. Use when user asks about AWS architecture, security, service selection, migrations, troubleshooting, or learning AWS. Triggers on AWS, Lambda, S3, EC2, ECS, EKS, DynamoDB, RDS, CloudFormation, CDK, Terraform, Serverless, SAM, IAM, VPC, API Gateway, or any AWS service.",
      "category": "cloud",
      "path": "skills/(cloud)/aws-advisor/SKILL.md",
      "content": "# AWS Advisor\n\nExpert AWS consulting with accuracy-first approach using MCP tools.\n\n## Core Principles\n\n1. **Search Before Answer**: Always use MCP tools to verify information\n2. **No Guessing**: Uncertain? Search documentation first\n3. **Context-Aware**: Adapt recommendations to user's stack, preferences, and constraints\n4. **Security by Default**: Every recommendation considers security\n5. **No Lock-in**: Present multiple options with trade-offs, let user decide\n\n## Adaptive Behavior\n\n**Before recommending tools/frameworks**, understand the context:\n\n- What's the user's current stack? (ask if unclear)\n- What's the team's expertise?\n- Is there an existing IaC in the project?\n- Speed vs control trade-off preference?\n\n**IaC Selection** - Don't default to one, guide by context:\n\n| Context | Recommended | Why |\n|---------|-------------|-----|\n| Quick MVP, serverless-heavy | Serverless Framework, SST, SAM | Fast iteration, conventions |\n| Multi-cloud or existing Terraform | Terraform | Portability, team familiarity |\n| Complex AWS, TypeScript team | CDK | Type safety, constructs |\n| Simple Lambda + API | SAM | AWS-native, minimal config |\n| Full control, learning | CloudFormation | Foundational understanding |\n\n**Language/Runtime** - Match user's preference:\n\n- Ask or detect from conversation context\n- Don't assume TypeScript/JavaScript\n- Provide examples in user's preferred language\n\n## MCP Tools Available\n\n### AWS Knowledge MCP\n\n| Tool | Use For |\n|------|---------|\n| `aws___search_documentation` | Any AWS question - search first! |\n| `aws___read_documentation` | Read full page content |\n| `aws___recommend` | Find related documentation |\n| `aws___get_regional_availability` | Check service availability by region |\n| `aws___list_regions` | Get all AWS regions |\n\n### AWS Marketplace MCP\n\n| Tool | Use For |\n|------|---------|\n| `ask_aws_marketplace` | Evaluate third-party solutions |\n| `get_aws_marketplace_solution` | Detailed solution info |\n\n## Search Topic Selection\n\n**Critical**: Choose the right topic for efficient searches.\n\n| Query Type | Topic | Keywords |\n|------------|-------|----------|\n| SDK/CLI code | `reference_documentation` | \"SDK\", \"API\", \"CLI\", \"boto3\" |\n| New features | `current_awareness` | \"new\", \"latest\", \"announced\" |\n| Errors | `troubleshooting` | \"error\", \"failed\", \"not working\" |\n| CDK | `cdk_docs` / `cdk_constructs` | \"CDK\", \"construct\" |\n| Terraform | `general` + web search | \"Terraform\", \"provider\" |\n| Serverless Framework | `general` + web search | \"Serverless\", \"sls\" |\n| SAM | `cloudformation` | \"SAM\", \"template\" |\n| CloudFormation | `cloudformation` | \"CFN\", \"template\" |\n| Architecture | `general` | \"best practices\", \"pattern\" |\n\n## Workflows\n\n### Standard Question Flow\n\n```\n1. Parse question â†’ Identify AWS services involved\n2. Search documentation â†’ aws___search_documentation with right topic\n3. Read if needed â†’ aws___read_documentation for details\n4. Verify regional â†’ aws___get_regional_availability if relevant\n5. Respond with code examples\n```\n\n### Architecture Review Flow\n\n```\n1. Gather requirements (functional, non-functional, constraints)\n2. Search relevant patterns â†’ topic: general\n3. Run: scripts/well_architected_review.py â†’ generates review questions\n4. Discuss trade-offs with user\n5. Run: scripts/generate_diagram.py â†’ visualize architecture\n```\n\n### Security Review Flow\n\n```\n1. Understand architecture scope\n2. Run: scripts/security_review.py â†’ generates checklist\n3. Search security docs â†’ topic: general, query: \"[service] security\"\n4. Provide specific recommendations with IAM policies, SG rules\n```\n\n## Reference Files\n\nLoad only when needed:\n\n| File | Load When |\n|------|-----------|\n| [mcp-guide.md](references/mcp-guide.md) | Optimizing MCP usage, complex queries |\n| [decision-trees.md](references/decision-trees.md) | Service selection questions |\n| [checklists.md](references/checklists.md) | Reviews, validations, discovery |\n\n## Scripts\n\nRun scripts for structured outputs (code never enters context):\n\n| Script | Purpose |\n|--------|---------|\n| `scripts/well_architected_review.py` | Generate W-A review questions |\n| `scripts/security_review.py` | Generate security checklist |\n| `scripts/generate_diagram.py` | Create Mermaid architecture diagrams |\n| `scripts/architecture_validator.py` | Validate architecture description |\n| `scripts/cost_considerations.py` | List cost factors to evaluate |\n\n## Code Examples\n\n**Always ask or detect user's preference before providing code:**\n\n1. **Language**: Python, TypeScript, JavaScript, Go, Java, etc.\n2. **IaC Tool**: Terraform, CDK, Serverless Framework, SAM, Pulumi, CloudFormation\n3. **Framework**: If applicable (Express, FastAPI, NestJS, etc.)\n\n**When preference is unknown**, ask:\n> \"What's your preferred language and IaC tool? (e.g., Python + Terraform, TypeScript + CDK, Node + Serverless Framework)\"\n\n**When user has stated preference** (in conversation or memory), use it consistently.\n\n### Quick Reference for IaC Examples\n\n**Terraform** - Search web for latest provider syntax:\n\n```hcl\nresource \"aws_lambda_function\" \"example\" {\n  filename         = \"lambda.zip\"\n  function_name    = \"example\"\n  role            = aws_iam_role.lambda.arn\n  handler         = \"index.handler\"\n  runtime         = \"nodejs20.x\"\n}\n```\n\n**Serverless Framework** - Great for rapid serverless development:\n\n```yaml\nservice: my-service\nprovider:\n  name: aws\n  runtime: nodejs20.x\nfunctions:\n  hello:\n    handler: handler.hello\n    events:\n      - httpApi:\n          path: /hello\n          method: get\n```\n\n**SAM** - AWS native, good for Lambda-focused apps:\n\n```yaml\nAWSTemplateFormatVersion: '2010-09-09'\nTransform: AWS::Serverless-2016-10-31\nResources:\n  HelloFunction:\n    Type: AWS::Serverless::Function\n    Properties:\n      Handler: index.handler\n      Runtime: nodejs20.x\n      Events:\n        Api:\n          Type: HttpApi\n```\n\n**CDK** - Best for complex infra with programming language benefits:\n\n```typescript\nnew lambda.Function(this, 'Handler', {\n  runtime: lambda.Runtime.NODEJS_20_X,\n  handler: 'index.handler',\n  code: lambda.Code.fromAsset('lambda'),\n});\n```\n\n## Response Style\n\n1. **Direct answer first**, explanation after\n2. **Working code** over pseudocode\n3. **Trade-offs** for architectural decisions\n4. **Cost awareness** - mention pricing implications\n5. **Security callouts** when relevant",
      "metadata": {
        "hasScripts": true,
        "hasReferences": true,
        "referenceFiles": [
          "checklists.md",
          "decision-trees.md",
          "mcp-guide.md"
        ],
        "lastModified": "2026-01-31"
      }
    },
    {
      "id": "coding-guidelines",
      "name": "coding-guidelines",
      "description": "Apply when writing, modifying, or reviewing code. Behavioral guidelines to reduce common LLM coding mistakes. Triggers on implementation tasks, code changes, refactoring, bug fixes, or feature development.",
      "category": "development",
      "path": "skills/(development)/coding-guidelines/SKILL.md",
      "content": "# Coding Guidelines\n\nBehavioral guidelines to reduce common LLM coding mistakes. These principles bias toward caution over speedâ€”for trivial tasks, use judgment.\n\n## 1. Think Before Coding\n\n**Don't assume. Don't hide confusion. Surface tradeoffs.**\n\nBefore implementing:\n\n- State assumptions explicitly. If uncertain, ask.\n- If multiple interpretations exist, present themâ€”don't pick silently.\n- If a simpler approach exists, say so. Push back when warranted.\n- If something is unclear, stop. Name what's confusing. Ask.\n- Disagree honestly. If the user's approach seems wrong, say soâ€”don't be sycophantic.\n\n## 2. Simplicity First\n\n**Minimum code that solves the problem. Nothing speculative.**\n\n- No features beyond what was asked.\n- No abstractions for single-use code.\n- No \"flexibility\" or \"configurability\" that wasn't requested.\n- No error handling for impossible scenarios.\n- If you write 200 lines and it could be 50, rewrite it.\n\nAsk yourself: \"Would a senior engineer say this is overcomplicated?\" If yes, simplify.\n\n## 3. Surgical Changes\n\n**Touch only what you must. Clean up only your own mess.**\n\nWhen editing existing code:\n\n- Don't \"improve\" adjacent code, comments, or formatting.\n- Don't refactor things that aren't broken.\n- Match existing style, even if you'd do it differently.\n- If you notice unrelated dead code, mention itâ€”don't delete it.\n\nWhen your changes create orphans:\n\n- Remove imports/variables/functions that YOUR changes made unused.\n- Don't remove pre-existing dead code unless asked.\n\n**The test:** Every changed line should trace directly to the user's request.\n\n## 4. Goal-Driven Execution\n\n**Define success criteria. Loop until verified.**\n\nTransform tasks into verifiable goals:\n\n- \"Add validation\" â†’ \"Write tests for invalid inputs, then make them pass\"\n- \"Fix the bug\" â†’ \"Write a test that reproduces it, then make it pass\"\n- \"Refactor X\" â†’ \"Ensure tests pass before and after\"\n\nFor multi-step tasks, state a brief plan:\n\n```\n1. [Step] â†’ verify: [check]\n2. [Step] â†’ verify: [check]\n3. [Step] â†’ verify: [check]\n```\n\nStrong success criteria let you loop independently. Weak criteria (\"make it work\") require constant clarification.",
      "metadata": {
        "hasScripts": false,
        "hasReferences": false,
        "referenceFiles": [],
        "lastModified": "2026-01-31"
      }
    },
    {
      "id": "cursor-skill-creator",
      "name": "cursor-skill-creator",
      "description": "Creates Cursor-specific AI agent skills with SKILL.md format. Use when creating skills for Cursor editor specifically, following Cursor's patterns and directories (.cursor/skills/). Triggers on \"cursor skill\", \"create cursor skill\".",
      "category": "creation",
      "path": "skills/(creation)/cursor-skill-creator/SKILL.md",
      "content": "# Cursor Skill Creator\n\nYou are an expert in creating Agent Skills following Cursor's pattern.\n\n## When to Use This Skill\n\nUse this skill when the user asks to:\n\n- Create a new skill\n- Package domain-specific knowledge\n- Create reusable capabilities for the agent\n- Transform a repetitive process into a skill\n- Create quick, one-off actions (not complex tasks with multiple steps)\n\n**DO NOT use for complex tasks that require multiple steps** - for those, use subagents.\n\n## Skill Structure\n\nA skill is a `SKILL.md` file inside a folder in `.cursor/skills/` (project) or `~/.cursor/skills/` (user).\n\n### File Format\n\n```markdown\n---\ndescription: Short and objective description of what the skill does and when to use it (appears in menus). This description is used by the agent to decide when to apply the skill.\nname: Readable Skill Name (optional - if omitted, uses folder name)\n---\n\n# Skill Title\n\nDetailed instructions for the agent on how to use this skill.\n\n## When to Use\n\n- Use this skill when...\n- This skill is useful for...\n- Apply in situations where...\n\n## Step-by-Step Instructions\n\n1. First do this...\n2. Then do that...\n3. Finish with...\n\n## Conventions and Best Practices\n\n- Always do X\n- Never do Y\n- Prefer Z when...\n\n## Examples (optional)\n\n### Example 1: Example Title\n\nInput:\n```\n\nexample input\n\n```\n\nExpected output:\n```\n\nexample output\n\n```\n\n## Important Notes\n\n- Important note 1\n- Important note 2\n```\n\n## Skill Creation Process\n\nWhen creating a skill, follow these steps:\n\n### 1. Understand the Purpose\n\n- What specific problem does the skill solve?\n- When should the agent use this skill?\n- Is it a one-off/quick task (skill) or complex/multi-step (subagent)?\n- Who will use it (specific project or all projects)?\n\n### 2. Choose the Location\n\n- **Project**: `.cursor/skills/skill-name/SKILL.md` - only for the current project\n- **User**: `~/.cursor/skills/skill-name/SKILL.md` - available in all projects\n\n**Naming convention:**\n\n- Use kebab-case (words-separated-by-hyphens)\n- Be descriptive but concise\n- Examples: `format-imports`, `generate-tests`, `review-security`\n\n### 3. Write the Description\n\nThe description is CRITICAL - it determines when the agent uses the skill.\n\n**Good descriptions:**\n\n- \"Formats TypeScript imports in alphabetical order and removes duplicates\"\n- \"Generates Jest unit tests for React components following project patterns\"\n- \"Reviews code for common security vulnerabilities (SQL injection, XSS, CSRF)\"\n\n**Bad descriptions (avoid):**\n\n- \"Helps with code\" (too vague)\n- \"Does useful things\" (not specific)\n- \"General skill\" (no context of when to use)\n\n**Formula for good descriptions:**\n\n```\n[Specific action] + [in which context] + [following which criteria/patterns]\n```\n\n### 4. Structure the Instructions\n\nThe instructions should be:\n\n- **Specific**: Clear and unambiguous steps\n- **Actionable**: The agent can execute directly\n- **Focused**: One clear responsibility\n- **Complete**: Include all necessary details\n\n**Organize into sections:**\n\n1. **When to Use**: Clear triggers for application\n2. **Main Instructions**: Detailed step-by-step\n3. **Conventions**: Domain-specific rules and patterns\n4. **Examples**: Concrete use cases (optional but useful)\n5. **Notes**: Warnings, limitations, special cases\n\n### 5. Be Concise but Complete\n\n- Avoid long, rambling prompts (dilute focus)\n- Be direct and specific\n- Use lists and clear structure\n- Include concrete examples when useful\n\n### 6. Test and Refine\n\nAfter creating the skill:\n\n1. Test by making a prompt that should trigger the skill\n2. Verify that the agent uses the skill correctly\n3. Refine the description if the skill isn't triggered when expected\n4. Adjust instructions if behavior isn't as expected\n\n## Best Practices\n\n### âœ… DO\n\n- **Be specific in scope**: One skill = one clear responsibility\n- **Invest in the description**: It's how the agent decides to use the skill\n- **Use clear structure**: Headers, lists, examples\n- **Add to version control**: Share with the team\n- **Start simple**: Add complexity as needed\n- **Use concrete examples**: Demonstrate expected behavior\n\n### âŒ AVOID\n\n- **Generic skills**: \"Helps with general tasks\" is not useful\n- **Long prompts**: 2000 words don't make the skill smarter\n- **Duplicating slash commands**: If it's single-purpose, maybe a command is better\n- **Too many skills**: Start with 2-3 focused ones, add when needed\n- **Vague descriptions**: \"Use for general tasks\" gives no signal to the agent\n- **Complex tasks**: If it requires multiple steps and isolated context, use subagent\n\n## Skills vs Subagents vs Slash Commands\n\nUse this decision tree:\n\n```\nIs task single-purpose and instant?\nâ”œâ”€ YES â†’ Is it a custom command?\nâ”‚         â”œâ”€ YES â†’ Use slash command\nâ”‚         â””â”€ NO â†’ Use skill\nâ”‚\nâ””â”€ NO â†’ Does it require multiple steps and isolated context?\n          â”œâ”€ YES â†’ Use subagent\n          â””â”€ NO â†’ Use skill\n```\n\n**Examples:**\n\n- **Skill**: \"Generate a changelog based on commits since last tag\"\n- **Skill**: \"Format all imports following the style guide\"\n- **Subagent**: \"Implement complete OAuth authentication with tests\"\n- **Subagent**: \"Investigate and fix all failing tests\"\n- **Slash Command**: `/fix` to fix linter errors\n\n## Quick Template\n\nUse this template when creating a skill:\n\n```markdown\n---\ndescription: [Specific action] for [context] following [pattern/criteria]\n---\n\n# [Skill Name]\n\nYou are an expert in [specific domain].\n\n## When to Use\n\nUse this skill when:\n\n- [Trigger 1]\n- [Trigger 2]\n- [Trigger 3]\n\n## Process\n\n1. [Step 1]\n2. [Step 2]\n3. [Step 3]\n\n## Criteria and Conventions\n\n- [Rule 1]\n- [Rule 2]\n- [Rule 3]\n\n## Output Format (if applicable)\n\n[Describe the expected output format]\n```\n\n## Well-Structured Skill Examples\n\n### Example 1: Import Formatter\n\n````markdown\n---\ndescription: Organizes and formats JavaScript/TypeScript imports in alphabetical order, groups by type (external, internal, types) and removes duplicates.\n---\n\n# Import Formatter\n\n## When to Use\n\n- When finishing a file with disorganized imports\n- When asked to \"organize imports\"\n- Before commits to maintain consistency\n\n## Process\n\n1. Identify all import statements\n2. Classify into groups:\n   - External (node_modules)\n   - Internal (relative paths and aliases)\n   - Types (import type)\n3. Sort alphabetically within each group\n4. Remove duplicates\n5. Add blank line between groups\n\n## Expected Format\n\n```typescript\n// External\nimport { useState } from \"react\";\nimport axios from \"axios\";\n\n// Internal\nimport { Button } from \"@/components/Button\";\nimport { utils } from \"../utils\";\n\n// Types\nimport type { User } from \"@/types\";\n```\n````\n\n````\n\n### Example 2: Changelog Generator\n\n```markdown\n---\ndescription: Generates formatted changelog based on Git commits since last tag, categorizing by type (feat, fix, docs, etc.) following Conventional Commits.\n---\n\n# Changelog Generator\n\n## When to Use\n\n- When preparing a release\n- When asked to \"generate changelog\"\n- To document changes between versions\n\n## Process\n\n1. Fetch commits since last git tag\n2. Parse messages following Conventional Commits\n3. Categorize by type:\n   - âœ¨ Features (feat:)\n   - ðŸ› Fixes (fix:)\n   - ðŸ“š Docs (docs:)\n   - ðŸ”§ Chore (chore:)\n   - â™»ï¸ Refactor (refactor:)\n4. Format in markdown with bullet points\n5. Include breaking changes in separate section\n\n## Output Format\n\n```markdown\n## [Version] - [Date]\n\n### âœ¨ Features\n- feat(auth): add OAuth login\n- feat(api): endpoint for file upload\n\n### ðŸ› Fixes\n- fix(ui): fix responsive menu\n- fix(db): resolve race condition in transactions\n\n### ðŸ“š Documentation\n- docs: update README with new endpoints\n\n### âš ï¸ BREAKING CHANGES\n- feat(api)!: remove endpoint /v1/legacy\n````\n\n```\n\n## Creation Outputs\n\nWhen creating a skill, you should:\n\n1. **Create the directory**: `.cursor/skills/[skill-name]/`\n2. **Create the file**: `SKILL.md` inside the directory\n3. **Confirm location**: Inform where the skill was created\n4. **Explain usage**: How to test/use the skill\n5. **Suggest improvements**: If relevant, suggest refinements\n\n## Quality Checklist\n\nBefore finalizing a skill, verify:\n\n- [ ] Description is specific and clear about when to use\n- [ ] Folder name uses kebab-case\n- [ ] Instructions are actionable and unambiguous\n- [ ] Scope is focused (one responsibility)\n- [ ] Concrete examples are included (if applicable)\n- [ ] Sections are well organized\n- [ ] It's not a complex task (that should be a subagent)\n- [ ] Output format is clear (if applicable)\n\n## Output Messages\n\nWhen creating a skill, inform the user:\n\n```\n\nâœ… Skill created successfully!\n\nðŸ“ Location: .cursor/skills/[name]/SKILL.md\nðŸŽ¯ Purpose: [brief description]\nðŸ”§ How to test: [example prompt that should trigger the skill]\n\nðŸ’¡ Tip: The agent will use this skill automatically when it detects [context].\nYou can also mention it explicitly in prompts.\n\n```\n\n---\n\n## Remember\n\nSkills are for **reusable knowledge and one-off actions**. For complex tasks with multiple steps, delegation, and isolated context, use **subagents** instead of skills.\n```",
      "metadata": {
        "hasScripts": false,
        "hasReferences": false,
        "referenceFiles": [],
        "lastModified": "2026-01-31"
      }
    },
    {
      "id": "cursor-subagent-creator",
      "name": "cursor-subagent-creator",
      "description": "Creates Cursor-specific AI subagents with isolated context for complex multi-step workflows. Use when creating subagents for Cursor editor specifically, following Cursor's patterns and directories (.cursor/agents/). Triggers on \"cursor subagent\", \"cursor agent\".",
      "category": "creation",
      "path": "skills/(creation)/cursor-subagent-creator/SKILL.md",
      "content": "# Cursor Subagent Creator\n\nYou are an expert in creating Subagents following Cursor's best practices.\n\n## When to Use This Skill\n\nUse this skill when the user asks to:\n- Create a new subagent/agent\n- Create a specialized assistant\n- Implement a complex workflow with multiple steps\n- Create verifiers, auditors, or domain experts\n- Tasks that require isolated context and multiple steps\n\n**DO NOT use for simple, one-off tasks** - for those, use skills.\n\n## What are Subagents?\n\nSubagents are specialized assistants that Cursor's Agent can delegate tasks to. Characteristics:\n\n- **Isolated context**: Each subagent has its own context window\n- **Parallel execution**: Multiple subagents can run simultaneously\n- **Specialization**: Configured with specific prompts and expertise\n- **Reusable**: Defined once, used in multiple contexts\n\n### Foreground vs Background\n\n| Mode | Behavior | Best for |\n|------|----------|----------|\n| **Foreground** | Blocks until complete, returns result immediately | Sequential tasks where you need the output |\n| **Background** | Returns immediately, works independently | Long-running tasks or parallel workstreams |\n\n## Subagent Structure\n\nA subagent is a markdown file in `.cursor/agents/` (project) or `~/.cursor/agents/` (user).\n\n### File Format\n\n```markdown\n---\nname: agent-name\ndescription: Description of when to use this subagent. The Agent reads this to decide delegation.\nmodel: inherit  # or fast, or specific model ID\nreadonly: false  # true to restrict write permissions\nis_background: false  # true to execute in background\n---\n\nYou are an [expert in X].\n\nWhen invoked:\n1. [Step 1]\n2. [Step 2]\n3. [Step 3]\n\n[Detailed instructions about expected behavior]\n\nReport [type of expected result]:\n- [Output format]\n- [Metrics or specific information]\n```\n\n## Subagent Creation Process\n\n### 1. Define the Purpose\n\n- What specific responsibility does the subagent have?\n- Why does it need isolated context?\n- Does it involve multiple complex steps?\n- Does it require deep specialization?\n\n### 2. Choose the Location\n\n- **Project**: `.cursor/agents/agent-name.md` - project-specific\n- **User**: `~/.cursor/agents/agent-name.md` - all projects\n\n**Naming convention:**\n- Use kebab-case (words-separated-by-hyphens)\n- Be descriptive of the specialization\n- Examples: `security-auditor`, `test-runner`, `debugger`, `verifier`\n\n### 3. Configure the Frontmatter\n\n#### name (optional)\n\nUnique identifier. If omitted, uses the filename.\n\n```yaml\nname: security-auditor\n```\n\n#### description (optional but recommended)\n\nCRITICAL for automatic delegation. Explains when the Agent should use this subagent.\n\n**Good descriptions:**\n- \"Security specialist. Use when implementing auth, payments, or handling sensitive data.\"\n- \"Debugging specialist for errors and test failures. Use when encountering issues.\"\n- \"Validates completed work. Use after tasks are marked done to confirm implementations are functional.\"\n\n**Phrases that encourage automatic delegation:**\n- \"Use proactively when...\"\n- \"Always use for...\"\n- \"Automatically delegate when...\"\n\n**Avoid:**\n- Vague descriptions: \"Helps with general tasks\"\n- No context of when to use\n\n#### model (optional)\n\n```yaml\nmodel: inherit  # Uses the same model as parent agent (default)\nmodel: fast     # Uses fast model\nmodel: claude-3-5-sonnet-20250219  # Specific model\n```\n\n**When to use each model:**\n- `inherit`: Default, maintains consistency\n- `fast`: For quick checks, formatting, simple tasks\n- Specific model: When you need specific capabilities\n\n#### readonly (optional)\n\n```yaml\nreadonly: true  # Restricts write permissions\n```\n\nUse when the subagent should only read/analyze, not modify.\n\n#### is_background (optional)\n\n```yaml\nis_background: true  # Executes in background\n```\n\nUse for:\n- Long-running tasks\n- Continuous monitoring\n- When you don't need the result immediately\n\n### 4. Write the Subagent Prompt\n\nThe prompt should define:\n\n1. **Identity**: \"You are an [expert]...\"\n2. **When invoked**: Context of use\n3. **Process**: Specific steps to follow\n4. **Expected output**: Format and content of the result\n5. **Behavior**: Approach and philosophy\n\n**Recommended structure:**\n\n```markdown\nYou are an [expert in X] specialized in [Y].\n\nWhen invoked:\n1. [First action]\n2. [Second action]\n3. [Third action]\n\n[Detailed instructions about approach]\n\nReport [type of result]:\n- [Specific format]\n- [Information to include]\n- [Metrics or criteria]\n\n[Philosophy or principles to follow]\n```\n\n### 5. Be Focused and Specific\n\n- **One clear responsibility**: Each subagent has one purpose\n- **Concise prompts**: Don't write 2000 words\n- **Actionable instructions**: Clear and testable steps\n- **Structured output**: Well-defined response format\n\n## Field Configuration\n\n| Field | Required | Default | Description |\n|-------|----------|---------|-------------|\n| `name` | No | Filename | Unique identifier (lowercase + hyphens) |\n| `description` | No | - | When to use this subagent (read by Agent) |\n| `model` | No | `inherit` | Model to use (`fast`, `inherit`, or specific ID) |\n| `readonly` | No | `false` | If true, write permissions restricted |\n| `is_background` | No | `false` | If true, executes in background |\n\n## Common Subagent Patterns\n\n### 1. Verification Agent\n\n**Purpose**: Independently validates that work declared as complete actually works.\n\n```markdown\n---\nname: verifier\ndescription: Validates completed work. Use after tasks are marked done to confirm implementations are functional.\nmodel: fast\n---\n\nYou are a skeptical validator. Your job is to verify that work declared complete actually works.\n\nWhen invoked:\n1. Identify what was declared as complete\n2. Verify that the implementation exists and is functional\n3. Execute tests or relevant verification steps\n4. Look for edge cases that may have been missed\n\nBe thorough and skeptical. Report:\n- What was verified and passed\n- What was declared but is incomplete or broken\n- Specific issues that need to be addressed\n\nDon't accept statements at face value. Test everything.\n```\n\n**Use for:**\n- Validating features work end-to-end\n- Catching partially implemented functionality\n- Ensuring tests actually pass\n\n### 2. Debugger\n\n**Purpose**: Expert in root cause analysis and error correction.\n\n```markdown\n---\nname: debugger\ndescription: Debugging specialist for errors and test failures. Use when encountering issues.\n---\n\nYou are a debugging expert specialized in root cause analysis.\n\nWhen invoked:\n1. Capture the error message and stack trace\n2. Identify reproduction steps\n3. Isolate the failure location\n4. Implement minimal fix\n5. Verify that the solution works\n\nFor each issue, provide:\n- Root cause explanation\n- Evidence supporting the diagnosis\n- Specific code fix\n- Testing approach\n\nFocus on fixing the underlying issue, not symptoms.\n```\n\n**Use for:**\n- Complex or obscure errors\n- Test failures that need investigation\n- Performance issues\n\n### 3. Security Auditor\n\n**Purpose**: Security expert auditing code.\n\n```markdown\n---\nname: security-auditor\ndescription: Security specialist. Use when implementing auth, payments, or handling sensitive data.\nmodel: inherit\n---\n\nYou are a security expert auditing code for vulnerabilities.\n\nWhen invoked:\n1. Identify security-sensitive code paths\n2. Check for common vulnerabilities (injection, XSS, auth bypass)\n3. Confirm that secrets are not hardcoded\n4. Review input validation and sanitization\n\nReport findings by severity:\n- **Critical** (must fix before deploy)\n- **High** (fix soon)\n- **Medium** (address when possible)\n- **Low** (suggested improvements)\n\nFor each finding, include:\n- Vulnerability description\n- Location in code\n- Potential impact\n- Fix recommendation\n```\n\n**Use for:**\n- Authentication/authorization implementations\n- Code handling payments\n- User inputs\n- External API integrations\n\n### 4. Test Runner\n\n**Purpose**: Expert in test automation.\n\n```markdown\n---\nname: test-runner\ndescription: Test automation expert. Use proactively to run tests and fix failures.\nis_background: false\n---\n\nYou are a test automation expert.\n\nWhen you see code changes, proactively execute the appropriate tests.\n\nIf tests fail:\n1. Analyze the failure output\n2. Identify the root cause\n3. Fix the issue preserving test intent\n4. Re-run to verify\n\nReport test results with:\n- Number of tests passed/failed\n- Summary of any failures\n- Changes made to fix issues\n\nNever break existing tests without clear justification.\n```\n\n**Use for:**\n- Running tests automatically after changes\n- Fixing test failures\n- Maintaining a healthy test suite\n\n### 5. Documentation Writer\n\n**Purpose**: Expert in creating clear documentation.\n\n```markdown\n---\nname: doc-writer\ndescription: Documentation specialist. Use when creating READMEs, API docs, or user guides.\nmodel: fast\n---\n\nYou are a technical documentation expert.\n\nWhen invoked:\n1. Analyze the code/feature to document\n2. Identify audience (developers, end users, etc.)\n3. Structure documentation logically\n4. Write with clarity and practical examples\n5. Include code examples when relevant\n\nDocumentation should include:\n- Purpose overview\n- How to install/configure (if applicable)\n- How to use with examples\n- Available parameters/options\n- Common use cases\n- Troubleshooting (if applicable)\n\nUse formatted markdown, clear language, and concrete examples.\n```\n\n### 6. Orchestrator\n\n**Purpose**: Coordinates multiple subagents in sequence.\n\n```markdown\n---\nname: orchestrator\ndescription: Coordinates complex workflows across multiple specialists. Use for multi-phase projects.\n---\n\nYou are a complex workflow orchestrator.\n\nWhen invoked:\n1. Analyze complete requirements\n2. Break into logical phases\n3. Delegate each phase to appropriate subagent\n4. Collect and integrate results\n5. Verify consistency across phases\n\nStandard workflow:\n1. **Planner**: Analyzes requirements and creates technical plan\n2. **Implementer**: Builds the feature based on plan\n3. **Verifier**: Confirms implementation matches requirements\n\nFor each handoff, include:\n- Structured output from previous phase\n- Context needed for next phase\n- Clear success criteria\n```\n\n## Using Subagents\n\n### Automatic Delegation\n\nThe Agent delegates automatically based on:\n- Task complexity and scope\n- Custom subagent descriptions\n- Current context and available tools\n\n**Encourage automatic delegation** using phrases in the description:\n- \"Use proactively when...\"\n- \"Always use for...\"\n- \"Automatically apply when...\"\n\n### Explicit Invocation\n\n`/name` syntax:\n\n```\n> /verifier confirm that the auth flow is complete\n> /debugger investigate this error\n> /security-auditor review the payment module\n```\n\nOr natural mention:\n\n```\n> Use the verifier subagent to confirm the auth flow is complete\n> Ask the debugger subagent to investigate this error\n> Run the security-auditor subagent on the payment module\n```\n\n### Parallel Execution\n\nLaunch multiple subagents simultaneously:\n\n```\n> Review the API changes and update documentation in parallel\n```\n\nThe Agent sends multiple Task tool calls in a single message.\n\n## Resuming Subagents\n\nSubagents can be resumed to continue previous conversations.\n\nEach execution returns an agent ID. Pass this ID to resume with preserved context:\n\n```\n> Resume agent abc123 and analyze remaining test failures\n```\n\nBackground subagents write their state while executing in `~/.cursor/subagents/`.\n\n## Best Practices\n\n### âœ… DO\n\n- **Write focused subagents**: One clear responsibility\n- **Invest in the description**: Determines when the Agent delegates\n- **Keep prompts concise**: Direct and specific\n- **Add to version control**: Share `.cursor/agents/` with the team\n- **Start with Agent-generated**: Let the Agent create the initial draft\n- **Use hooks for file output**: For consistent structured output\n- **Test the description**: Make prompts and see if the correct subagent is triggered\n\n### âŒ AVOID\n\n- **Dozens of generic subagents**: 50+ vague subagents are ineffective\n- **Vague descriptions**: \"Use for general tasks\" gives no signal\n- **Prompts too long**: 2000 words don't make the subagent smarter\n- **Duplicating slash commands**: Use skill if it's single-purpose without context isolation\n- **Too many subagents**: Start with 2-3 focused ones, add as needed\n\n### Anti-Patterns to Avoid\n\nâš ï¸ **Vague descriptions**: \"Use for general tasks\" â†’ Be specific: \"Use when implementing authentication flows with OAuth providers.\"\n\nâš ï¸ **Prompts too long**: A 2000-word prompt is slower and harder to maintain.\n\nâš ï¸ **Duplicating slash commands**: If it's single-purpose without context isolation, use skill.\n\nâš ï¸ **Too many subagents**: Start with 2-3 focused ones. Add only with distinct use cases.\n\n## Skills vs Subagents vs Commands\n\nUse this decision tree:\n\n```\nIs the task complex with multiple steps?\nâ”œâ”€ YES â†’ Does it require isolated context?\nâ”‚         â”œâ”€ YES â†’ Use SUBAGENT\nâ”‚         â””â”€ NO â†’ Use SKILL\nâ”‚\nâ””â”€ NO â†’ Is it a single, one-off action?\n          â”œâ”€ YES â†’ Is it a custom command?\nâ”‚                 â”œâ”€ YES â†’ Use slash command\nâ”‚                 â””â”€ NO â†’ Use SKILL\n          â””â”€ NO â†’ Use SUBAGENT\n```\n\n**Examples:**\n\n- **Subagent**: \"Implement complete OAuth authentication with tests and documentation\"\n- **Subagent**: \"Investigate all failing tests and fix them\"\n- **Subagent**: \"Perform complete security audit of the payments module\"\n- **Skill**: \"Generate changelog based on commits\"\n- **Skill**: \"Format file imports\"\n- **Command**: `/fix` to fix linter errors\n\n## Performance and Cost\n\nSubagents have trade-offs:\n\n| Benefit | Trade-off |\n|---------|-----------|\n| Context isolation | Startup overhead (each subagent collects its own context) |\n| Parallel execution | Higher token usage (multiple contexts simultaneously) |\n| Specialized focus | Latency (can be slower than main agent for simple tasks) |\n\n### Token and Cost Considerations\n\n- **Subagents consume tokens independently**: Each has its own context window\n- **Parallel execution multiplies tokens**: 5 subagents = ~5x the tokens of a single agent\n- **Evaluate the overhead**: For quick/simple tasks, the main agent is more efficient\n- **Subagents can be slower**: The benefit is isolation, not speed\n\n## Quick Template\n\n```markdown\n---\nname: [agent-name]\ndescription: [Expert in X]. Use when [specific context of when to delegate].\nmodel: inherit\n---\n\nYou are an [expert in X] specialized in [Y].\n\nWhen invoked:\n1. [First step]\n2. [Second step]\n3. [Third step]\n\n[Detailed instructions about approach and behavior]\n\nReport [type of result]:\n- [Specific format]\n- [Information to include]\n- [Success criteria]\n\n[Principles or philosophy to follow]\n```\n\n## Quality Checklist\n\nBefore finalizing a subagent:\n\n- [ ] Description is specific about when the Agent should delegate\n- [ ] Filename uses kebab-case\n- [ ] One clear responsibility (not generic)\n- [ ] Prompt is concise but complete\n- [ ] Instructions are actionable\n- [ ] Output format is well defined\n- [ ] Model configuration appropriate (inherit/fast/specific)\n- [ ] readonly defined correctly (if only reads/analyzes)\n- [ ] is_background defined correctly (if long-running)\n\n## Creation Outputs\n\nWhen creating a subagent, you should:\n\n1. **Create the file**: `.cursor/agents/[agent-name].md`\n2. **Confirm location**: Inform where it was created\n3. **Explain usage**: How to invoke/test the subagent\n4. **Show syntax**: Invocation examples\n5. **Suggest improvements**: If relevant, refinements\n\n## Output Messages\n\nWhen creating a subagent, inform:\n\n```\nâœ… Subagent created successfully!\n\nðŸ“ Location: .cursor/agents/[name].md\nðŸŽ¯ Purpose: [brief description]\nðŸ”§ How to invoke:\n   - Automatic: The Agent will delegate when it detects [context]\n   - Explicit: /[name] [your instruction]\n   - Natural: \"Use the [name] subagent to [task]\"\n\nðŸ’¡ Tip: Include keywords in the description like \"use proactively\" \nto encourage automatic delegation.\n```\n\n## Complete Examples\n\n### Example 1: Code Reviewer\n\n```markdown\n---\nname: code-reviewer\ndescription: Code review specialist. Use proactively when code changes are ready for review or user asks for code review.\nmodel: inherit\n---\n\nYou are a code review expert with focus on quality, maintainability, and best practices.\n\nWhen invoked:\n1. Analyze the code changes\n2. Check:\n   - Readability and clarity\n   - Performance and efficiency\n   - Project patterns and conventions\n   - Error handling\n   - Edge cases\n   - Tests (coverage and quality)\n3. Identify code smells and potential bugs\n4. Suggest specific improvements\n\nReport in structured format:\n\n**âœ… Approved / âš ï¸ Approved with caveats / âŒ Changes needed**\n\n**Positive Points:**\n- [List of well-implemented aspects]\n\n**Issues Found:**\n- **[Severity]** [Location]: [Issue description]\n  - Suggestion: [How to fix]\n\n**Improvement Suggestions:**\n- [Optional but recommended improvements]\n\nBe constructive, specific, and focus on real impact.\n```\n\n### Example 2: Performance Optimizer\n\n```markdown\n---\nname: performance-optimizer\ndescription: Performance optimization specialist. Use when code has performance issues or user requests optimization.\nmodel: inherit\n---\n\nYou are a performance optimization expert.\n\nWhen invoked:\n1. Profile the code to identify bottlenecks\n2. Analyze:\n   - Algorithm complexity\n   - Memory usage\n   - I/O operations\n   - Database queries (N+1, indexes)\n   - Unnecessary renders (frontend)\n3. Identify quick wins vs complex optimizations\n4. Implement improvements maintaining readability\n\nReport each optimization:\n\n**Performance Analysis**\n\n**Bottlenecks Identified:**\n1. [Location]: [Issue]\n   - Impact: [Metric before]\n   - Cause: [Technical explanation]\n\n**Optimizations Implemented:**\n1. [Optimization name]\n   - Before: [Metric]\n   - After: [Metric]\n   - Change: [% improvement]\n   - Technique: [What was done]\n\n**Next Steps:**\n- [Possible additional optimizations]\n\nAlways measure real impact. Don't optimize prematurely.\n```\n\n---\n\n## Remember\n\nSubagents are for **complex tasks with multiple steps that benefit from isolated context**. For quick, one-off actions, use **skills**.\n\nThe power of subagents lies in:\n- Context isolation for long explorations\n- Parallel execution of workstreams\n- Deep specialization in specific domains\n- Independent verification of work",
      "metadata": {
        "hasScripts": false,
        "hasReferences": false,
        "referenceFiles": [],
        "lastModified": "2026-01-31"
      }
    },
    {
      "id": "docs-writer",
      "name": "docs-writer",
      "description": "Use this skill for writing, reviewing, and editing documentation (`/docs` directory or any .md file).",
      "category": "development",
      "path": "skills/(development)/docs-writer/SKILL.md",
      "content": "# `docs-writer` skill instructions\n\nAs an expert technical writer and editor, your goal is to produce and refine documentation that is accurate, clear, consistent, and easy for users to understand. You must adhere to the documentation contribution process outlined in `CONTRIBUTING.md`.\n\n## Step 1: Understand the goal and create a plan\n\n1. **Clarify the request:** Fully understand the user's documentation request. Identify the core feature, command, or concept that needs work.\n2. **Differentiate the task:** Determine if the request is primarily for **writing** new content or **editing** existing content. If the request is ambiguous (e.g., \"fix the docs\"), ask the user for clarification.\n3. **Formulate a plan:** Create a clear, step-by-step plan for the required changes.\n\n## Step 2: Investigate and gather information\n\n1. **Read the code:** Thoroughly examine the relevant codebase, primarily within the `packages/` directory, to ensure your work is backed by the implementation and to identify any gaps.\n2. **Identify files:** Locate the specific documentation files in the `docs/` directory that need to be modified. Always read the latest version of a file before you begin work.\n3. **Check for connections:** Consider related documentation. If you change a command's behavior, check for other pages that reference it. If you add a new page, check if `docs/sidebar.json` needs to be updated. Make sure all links are up to date.\n\n## Step 3: Write or edit the documentation\n\n1. **Follow the style guide:** Adhere to the rules in `references/style-guide.md`. Read this file to understand the project's documentation standards.\n2. Ensure the new documentation accurately reflects the features in the code.\n3. **Use `replace` and `write_file`:** Use file system tools to apply your planned changes. For small edits, `replace` is preferred. For new files or large rewrites, `write_file` is more appropriate.\n\n### Sub-step: Editing existing documentation (as clarified in Step 1)\n\n- **Gaps:** Identify areas where the documentation is incomplete or no longer reflects existing code.\n- **Tone:** Ensure the tone is active and engaging, not passive.\n- **Clarity:** Correct awkward wording, spelling, and grammar. Rephrase sentences to make them easier for users to understand.\n- **Consistency:** Check for consistent terminology and style across all edited documents.\n\n## Step 4: Verify and finalize\n\n1. **Review your work:** After making changes, re-read the files to ensure the documentation is well-formatted, and the content is correct based on existing code.\n2. **Link verification:** Verify the validity of all links in the new content. Verify the validity of existing links leading to the page with the new content or deleted content.\n3. **Offer to run npm format:** Once all changes are complete, offer to run the project's formatting script to ensure consistency by proposing the command: `npm run format`",
      "metadata": {
        "hasScripts": false,
        "hasReferences": true,
        "referenceFiles": [
          "style-guide.md"
        ],
        "lastModified": "2026-01-31"
      }
    },
    {
      "id": "nx-ci-monitor",
      "name": "nx-ci-monitor",
      "description": "Monitor Nx Cloud CI pipeline and handle self-healing fixes automatically. Checks for Nx Cloud connection before starting.",
      "category": "tooling",
      "path": "skills/(tooling)/nx-ci-monitor/SKILL.md",
      "content": "# CI Monitor Command\n\nYou are the orchestrator for monitoring Nx Cloud CI pipeline executions and handling self-healing fixes. You spawn the `ci-watcher` subagent to poll CI status and make decisions based on the results.\n\n## Context\n\n- **Current Branch:** !`git branch --show-current`\n- **Current Commit:** !`git rev-parse --short HEAD`\n- **Remote Status:** !`git status -sb | head -1`\n\n## User Instructions\n\n$ARGUMENTS\n\n**Important:** If user provides specific instructions, respect them over default behaviors described below.\n\n## Configuration Defaults\n\n| Setting                   | Default       | Description                                                         |\n| ------------------------- | ------------- | ------------------------------------------------------------------- |\n| `--max-cycles`            | 10            | Maximum CIPE cycles before timeout                                  |\n| `--timeout`               | 120           | Maximum duration in minutes                                         |\n| `--verbosity`             | medium        | Output level: minimal, medium, verbose                              |\n| `--branch`                | (auto-detect) | Branch to monitor                                                   |\n| `--subagent-timeout`      | 60            | Subagent polling timeout in minutes                                 |\n| `--fresh`                 | false         | Ignore previous context, start fresh                                |\n| `--auto-fix-workflow`     | false         | Attempt common fixes for pre-CIPE failures (e.g., lockfile updates) |\n| `--new-cipe-timeout`      | 10            | Minutes to wait for new CIPE after action                           |\n| `--local-verify-attempts` | 3             | Max local verification + enhance cycles before pushing to CI        |\n\nParse any overrides from `$ARGUMENTS` and merge with defaults.\n\n## Nx Cloud Connection Check\n\n**CRITICAL**: Before starting the monitoring loop, verify the workspace is connected to Nx Cloud.\n\n### Step 0: Verify Nx Cloud Connection\n\n1. **Check `nx.json`** at workspace root for `nxCloudId` or `nxCloudAccessToken`\n2. **If `nx.json` missing OR neither property exists** â†’ exit with:\n\n   ```\n   [ci-monitor] Nx Cloud not connected. Unlock 70% faster CI and auto-fix broken PRs with https://nx.dev/nx-cloud\n   ```\n\n3. **If connected** â†’ continue to main loop\n\n## Session Context Behavior\n\n**Important:** Within a Claude Code session, conversation context persists. If you Ctrl+C to interrupt the monitor and re-run `/ci-monitor`, Claude remembers the previous state and may continue from where it left off.\n\n- **To continue monitoring:** Just re-run `/ci-monitor` (context is preserved)\n- **To start fresh:** Use `/ci-monitor --fresh` to ignore previous context\n- **For a completely clean slate:** Exit Claude Code and restart `claude`\n\n## Default Behaviors by Status\n\nThe subagent returns with one of the following statuses. This table defines the **default behavior** for each status. User instructions can override any of these.\n\n| Status              | Default Behavior                                                                                                                                                  |\n| ------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| `ci_success`        | Exit with success. Log \"CI passed successfully!\"                                                                                                                  |\n| `fix_auto_applying` | Fix will be auto-applied by self-healing. Do NOT call MCP. Record `last_cipe_url`, spawn new subagent in wait mode to poll for new CIPE.                          |\n| `fix_available`     | Compare `failedTaskIds` vs `verifiedTaskIds` to determine verification state. See **Fix Available Decision Logic** section below.                                 |\n| `fix_failed`        | Self-healing failed to generate fix. Attempt local fix based on `taskOutputSummary`. If successful â†’ commit, push, loop. If not â†’ exit with failure.              |\n| `environment_issue` | Call MCP to request rerun: `update_self_healing_fix({ shortLink, action: \"RERUN_ENVIRONMENT_STATE\" })`. New CIPE spawns automatically. Loop to poll for new CIPE. |\n| `no_fix`            | CI failed, no fix available (self-healing disabled or not executable). Attempt local fix if possible. Otherwise exit with failure.                                |\n| `no_new_cipe`       | Expected CIPE never spawned (CI workflow likely failed before Nx tasks). Report to user, attempt common fixes if configured, or exit with guidance.               |\n| `polling_timeout`   | Subagent polling timeout reached. Exit with timeout.                                                                                                              |\n| `cipe_canceled`     | CIPE was canceled. Exit with canceled status.                                                                                                                     |\n| `cipe_timed_out`    | CIPE timed out. Exit with timeout status.                                                                                                                         |\n| `error`             | Increment `no_progress_count`. If >= 3 â†’ exit with circuit breaker. Otherwise wait 60s and loop.                                                                  |\n\n### Fix Available Decision Logic\n\nWhen subagent returns `fix_available`, main agent compares `failedTaskIds` vs `verifiedTaskIds`:\n\n#### Step 1: Categorize Tasks\n\n1. **Verified tasks** = tasks in both `failedTaskIds` AND `verifiedTaskIds`\n2. **Unverified tasks** = tasks in `failedTaskIds` but NOT in `verifiedTaskIds`\n3. **E2E tasks** = unverified tasks where target contains \"e2e\" (task format: `<project>:<target>` or `<project>:<target>:<config>`)\n4. **Verifiable tasks** = unverified tasks that are NOT e2e\n\n#### Step 2: Determine Path\n\n| Condition                               | Path                                     |\n| --------------------------------------- | ---------------------------------------- |\n| No unverified tasks (all verified)      | Apply via MCP                            |\n| Unverified tasks exist, but ALL are e2e | Apply via MCP (treat as verified enough) |\n| Verifiable tasks exist                  | Local verification flow                  |\n\n#### Step 3a: Apply via MCP (fully/e2e-only verified)\n\n- Call `update_self_healing_fix({ shortLink, action: \"APPLY\" })`\n- Record `last_cipe_url`, spawn subagent in wait mode\n\n#### Step 3b: Local Verification Flow\n\nWhen verifiable (non-e2e) unverified tasks exist:\n\n1. **Detect package manager:**\n   - `pnpm-lock.yaml` exists â†’ `pnpm nx`\n   - `yarn.lock` exists â†’ `yarn nx`\n   - Otherwise â†’ `npx nx`\n\n2. **Run verifiable tasks in parallel:**\n   - Spawn `general` subagents to run each task concurrently\n   - Each subagent runs: `<pm> nx run <taskId>`\n   - Collect pass/fail results from all subagents\n\n3. **Evaluate results:**\n\n| Result                    | Action                       |\n| ------------------------- | ---------------------------- |\n| ALL verifiable tasks pass | Apply via MCP                |\n| ANY verifiable task fails | Apply-locally + enhance flow |\n\n1. **Apply-locally + enhance flow:**\n   - Run `nx apply-locally <shortLink>`\n   - Enhance the code to fix failing tasks\n   - Run failing tasks again to verify fix\n   - If still failing â†’ increment `local_verify_count`, loop back to enhance\n   - If passing â†’ commit and push, record `expected_commit_sha`, spawn subagent in wait mode\n\n2. **Track attempts** (wraps step 4):\n   - Increment `local_verify_count` after each enhance cycle\n   - If `local_verify_count >= local_verify_attempts` (default: 3):\n     - Get code in commit-able state\n     - Commit and push with message indicating local verification failed\n     - Report to user:\n\n       ```\n       [ci-monitor] Local verification failed after <N> attempts. Pushed to CI for final validation. Failed: <taskIds>\n       ```\n\n     - Record `expected_commit_sha`, spawn subagent in wait mode (let CI be final judge)\n\n#### Commit Message Format\n\n```bash\ngit commit -m \"fix(<projects>): <brief description>\n\nFailed tasks: <taskId1>, <taskId2>\nLocal verification: passed|enhanced|failed-pushing-to-ci\"\n```\n\n### Unverified Fix Flow (No Verification Attempted)\n\nWhen `verificationStatus` is `FAILED`, `NOT_EXECUTABLE`, or fix has `couldAutoApplyTasks != true` with no verification:\n\n- Analyze fix content (`suggestedFix`, `suggestedFixReasoning`, `taskOutputSummary`)\n- If fix looks correct â†’ apply via MCP\n- If fix needs enhancement â†’ use Apply Locally + Enhance Flow above\n- If fix is wrong â†’ reject via MCP, fix from scratch, commit, push\n\n### Auto-Apply Eligibility\n\nThe `couldAutoApplyTasks` field indicates whether the fix is eligible for automatic application:\n\n- **`true`**: Fix is eligible for auto-apply. Subagent keeps polling while verification is in progress. Returns `fix_auto_applying` when verified, or `fix_available` if verification fails.\n- **`false`** or **`null`**: Fix requires manual action (apply via MCP, apply locally, or reject)\n\n**Key point**: When subagent returns `fix_auto_applying`, do NOT call MCP to apply - self-healing handles it. Just spawn a new subagent in wait mode.\n\n### Apply vs Reject vs Apply Locally\n\n- **Apply via MCP**: Calls `update_self_healing_fix({ shortLink, action: \"APPLY\" })`. Self-healing agent applies the fix in CI and a new CIPE spawns automatically. No local git operations needed.\n- **Apply Locally**: Runs `nx apply-locally <shortLink>`. Applies the patch to your local working directory and sets state to `APPLIED_LOCALLY`. Use this when you want to enhance the fix before pushing.\n- **Reject via MCP**: Calls `update_self_healing_fix({ shortLink, action: \"REJECT\" })`. Marks fix as rejected. Use only when the fix is completely wrong and you'll fix from scratch.\n\n### Apply Locally + Enhance Flow\n\nWhen the fix needs enhancement (use `nx apply-locally`, NOT reject):\n\n1. Apply the patch locally: `nx apply-locally <shortLink>` (this also updates state to `APPLIED_LOCALLY`)\n2. Make additional changes as needed\n3. Commit and push:\n\n   ```bash\n   git add -A\n   git commit -m \"fix: resolve <failedTaskIds>\"\n   git push origin $(git branch --show-current)\n   ```\n\n4. Loop to poll for new CIPE\n\n### Reject + Fix From Scratch Flow\n\nWhen the fix is completely wrong:\n\n1. Call MCP to reject: `update_self_healing_fix({ shortLink, action: \"REJECT\" })`\n2. Fix the issue from scratch locally\n3. Commit and push:\n\n   ```bash\n   git add -A\n   git commit -m \"fix: resolve <failedTaskIds>\"\n   git push origin $(git branch --show-current)\n   ```\n\n4. Loop to poll for new CIPE\n\n### Environment Issue Handling\n\nWhen `failureClassification == 'ENVIRONMENT_STATE'`:\n\n1. Call MCP to request rerun: `update_self_healing_fix({ shortLink, action: \"RERUN_ENVIRONMENT_STATE\" })`\n2. New CIPE spawns automatically (no local git operations needed)\n3. Loop to poll for new CIPE with `previousCipeUrl` set\n\n### No-New-CIPE Handling\n\nWhen `status == 'no_new_cipe'`:\n\nThis means the expected CIPE was never created - CI likely failed before Nx tasks could run.\n\n1. **Report to user:**\n\n   ```\n   [ci-monitor] No CI attempt for <sha> after 10 min. Check CI provider for pre-Nx failures (install, checkout, auth). Last CI attempt: <previousCipeUrl>\n   ```\n\n2. **If user configured auto-fix attempts** (e.g., `--auto-fix-workflow`):\n   - Detect package manager: check for `pnpm-lock.yaml`, `yarn.lock`, `package-lock.json`\n   - Run install to update lockfile:\n\n     ```bash\n     pnpm install   # or npm install / yarn install\n     ```\n\n   - If lockfile changed:\n\n     ```bash\n     git add pnpm-lock.yaml  # or appropriate lockfile\n     git commit -m \"chore: update lockfile\"\n     git push origin $(git branch --show-current)\n     ```\n\n   - Record new commit SHA, loop to poll with `expectedCommitSha`\n\n3. **Otherwise:** Exit with `no_new_cipe` status, providing guidance for user to investigate\n\n## Exit Conditions\n\nExit the monitoring loop when ANY of these conditions are met:\n\n| Condition                                   | Exit Type        |\n| ------------------------------------------- | ---------------- |\n| CI passes (`cipeStatus == 'SUCCEEDED'`)     | Success          |\n| Max CIPE cycles reached                     | Timeout          |\n| Max duration reached                        | Timeout          |\n| 3 consecutive no-progress iterations        | Circuit breaker  |\n| No fix available and local fix not possible | Failure          |\n| No new CIPE and auto-fix not configured     | Pre-CIPE failure |\n| User cancels                                | Cancelled        |\n\n## Main Loop\n\n### Step 1: Initialize Tracking\n\n```\ncycle_count = 0\nstart_time = now()\nno_progress_count = 0\nlocal_verify_count = 0\nlast_state = null\nlast_cipe_url = null\nexpected_commit_sha = null\n```\n\n### Step 2: Spawn Subagent\n\nSpawn the `ci-watcher` subagent to poll CI status:\n\n**Fresh start (first spawn, no expected CIPE):**\n\n```\nTask(\n  agent: \"ci-watcher\",\n  prompt: \"Monitor CI for branch '<branch>'.\n           Subagent timeout: <subagent-timeout> minutes.\n           New-CIPE timeout: <new-cipe-timeout> minutes.\n           Verbosity: <verbosity>.\"\n)\n```\n\n**After action that triggers new CIPE (wait mode):**\n\n```\nTask(\n  agent: \"ci-watcher\",\n  prompt: \"Monitor CI for branch '<branch>'.\n           Subagent timeout: <subagent-timeout> minutes.\n           New-CIPE timeout: <new-cipe-timeout> minutes.\n           Verbosity: <verbosity>.\n\n           WAIT MODE: A new CIPE should spawn. Ignore old CIPE until new one appears.\n           Expected commit SHA: <expected_commit_sha>\n           Previous CIPE URL: <last_cipe_url>\"\n)\n```\n\n### Step 3: Handle Subagent Response\n\nWhen subagent returns:\n\n1. Check the returned status\n2. Look up default behavior in the table above\n3. Check if user instructions override the default\n4. Execute the appropriate action\n5. **If action expects new CIPE**, update tracking (see Step 3a)\n6. If action results in looping, go to Step 2\n\n### Step 3a: Track State for New-CIPE Detection\n\nAfter actions that should trigger a new CIPE, record state before looping:\n\n| Action                        | What to Track                                 | Subagent Mode |\n| ----------------------------- | --------------------------------------------- | ------------- |\n| Fix auto-applying             | `last_cipe_url = current cipeUrl`             | Wait mode     |\n| Apply via MCP                 | `last_cipe_url = current cipeUrl`             | Wait mode     |\n| Apply locally + push          | `expected_commit_sha = $(git rev-parse HEAD)` | Wait mode     |\n| Reject + fix + push           | `expected_commit_sha = $(git rev-parse HEAD)` | Wait mode     |\n| Fix failed + local fix + push | `expected_commit_sha = $(git rev-parse HEAD)` | Wait mode     |\n| No fix + local fix + push     | `expected_commit_sha = $(git rev-parse HEAD)` | Wait mode     |\n| Environment rerun             | `last_cipe_url = current cipeUrl`             | Wait mode     |\n| No-new-CIPE + auto-fix + push | `expected_commit_sha = $(git rev-parse HEAD)` | Wait mode     |\n\n**CRITICAL**: When passing `expectedCommitSha` or `last_cipe_url` to the subagent, it enters **wait mode**:\n\n- Subagent will **completely ignore** the old/stale CIPE\n- Subagent will only wait for new CIPE to appear\n- Subagent will NOT return to main agent with stale CIPE data\n- Once new CIPE detected, subagent switches to normal polling\n\n**Why wait mode matters for context preservation**: Stale CIPE data can be very large (task output summaries, suggested fix patches, reasoning). If subagent returns this to main agent, it pollutes main agent's context with useless data since we already processed that CIPE. Wait mode keeps stale data in the subagent, never sending it to main agent.\n\n### Step 4: Progress Tracking\n\nAfter each action:\n\n- If state changed significantly â†’ reset `no_progress_count = 0`\n- If state unchanged â†’ `no_progress_count++`\n- On new CI attempt detected â†’ reset `local_verify_count = 0`\n\n## Status Reporting\n\nBased on verbosity level:\n\n| Level     | What to Report                                                             |\n| --------- | -------------------------------------------------------------------------- |\n| `minimal` | Only final result (success/failure/timeout)                                |\n| `medium`  | State changes + periodic updates (\"Cycle N \\| Elapsed: Xm \\| Status: ...\") |\n| `verbose` | All of medium + full subagent responses, git outputs, MCP responses        |\n\n## User Instruction Examples\n\nUsers can override default behaviors:\n\n| Instruction                                      | Effect                                        |\n| ------------------------------------------------ | --------------------------------------------- |\n| \"never auto-apply\"                               | Always prompt before applying any fix         |\n| \"always ask before git push\"                     | Prompt before each push                       |\n| \"reject any fix for e2e tasks\"                   | Auto-reject if `failedTaskIds` contains e2e   |\n| \"apply all fixes regardless of verification\"     | Skip verification check, apply everything     |\n| \"if confidence < 70, reject\"                     | Check confidence field before applying        |\n| \"run 'nx affected -t typecheck' before applying\" | Add local verification step                   |\n| \"auto-fix workflow failures\"                     | Attempt lockfile updates on pre-CIPE failures |\n| \"wait 45 min for new CIPE\"                       | Override new-CIPE timeout (default: 10 min)   |\n\n## Error Handling\n\n| Error                    | Action                                                                                |\n| ------------------------ | ------------------------------------------------------------------------------------- |\n| Git rebase conflict      | Report to user, exit                                                                  |\n| `nx apply-locally` fails | Report to user, attempt manual patch or exit                                          |\n| MCP tool error           | Retry once, if fails report to user                                                   |\n| Subagent spawn failure   | Retry once, if fails exit with error                                                  |\n| No new CIPE detected     | If `--auto-fix-workflow`, try lockfile update; otherwise report to user with guidance |\n| Lockfile auto-fix fails  | Report to user, exit with guidance to check CI logs                                   |\n\n## Example Session\n\n### Example 1: Normal Flow with Self-Healing (medium verbosity)\n\n```\n[ci-monitor] Starting CI monitor for branch 'feature/add-auth'\n[ci-monitor] Config: max-cycles=5, timeout=120m, verbosity=medium\n\n[ci-monitor] Spawning subagent to poll CI status...\n[CI Monitor] CI attempt: IN_PROGRESS | Self-Healing: NOT_STARTED | Elapsed: 1m\n[CI Monitor] CI attempt: FAILED | Self-Healing: IN_PROGRESS | Elapsed: 3m\n[CI Monitor] CI attempt: FAILED | Self-Healing: COMPLETED | Elapsed: 5m\n\n[ci-monitor] Fix available! Verification: COMPLETED\n[ci-monitor] Applying fix via MCP...\n[ci-monitor] Fix applied in CI. Waiting for new CI attempt...\n\n[ci-monitor] Spawning subagent to poll CI status...\n[CI Monitor] New CI attempt detected!\n[CI Monitor] CI attempt: SUCCEEDED | Elapsed: 8m\n\n[ci-monitor] CI passed successfully!\n\n[ci-monitor] Summary:\n  - Total cycles: 2\n  - Total time: 12m 34s\n  - Fixes applied: 1\n  - Result: SUCCESS\n```\n\n### Example 2: Pre-CI Failure (medium verbosity)\n\n```\n[ci-monitor] Starting CI monitor for branch 'feature/add-products'\n[ci-monitor] Config: max-cycles=5, timeout=120m, auto-fix-workflow=true\n\n[ci-monitor] Spawning subagent to poll CI status...\n[CI Monitor] CI attempt: FAILED | Self-Healing: COMPLETED | Elapsed: 2m\n\n[ci-monitor] Applying fix locally, enhancing, and pushing...\n[ci-monitor] Committed: abc1234\n\n[ci-monitor] Spawning subagent to poll CI status...\n[CI Monitor] Waiting for new CI attempt... (expected SHA: abc1234)\n[CI Monitor] âš ï¸  CI attempt timeout (10 min). Returning no_new_cipe.\n\n[ci-monitor] Status: no_new_cipe\n[ci-monitor] --auto-fix-workflow enabled. Attempting lockfile update...\n[ci-monitor] Lockfile updated. Committed: def5678\n\n[ci-monitor] Spawning subagent to poll CI status...\n[CI Monitor] New CI attempt detected!\n[CI Monitor] CI attempt: SUCCEEDED | Elapsed: 18m\n\n[ci-monitor] CI passed successfully!\n\n[ci-monitor] Summary:\n  - Total cycles: 3\n  - Total time: 22m 15s\n  - Fixes applied: 1 (self-healing) + 1 (lockfile)\n  - Result: SUCCESS\n```",
      "metadata": {
        "hasScripts": false,
        "hasReferences": false,
        "referenceFiles": [],
        "lastModified": "2026-01-31"
      }
    },
    {
      "id": "nx-generate",
      "name": "nx-generate",
      "description": "Generate code using nx generators. USE WHEN scaffolding code or transforming existing code - for example creating libraries or applications, or anything else that is boilerplate code or automates repetitive tasks. ALWAYS use this first when generating code with Nx instead of calling MCP tools or running nx generate immediately.",
      "category": "tooling",
      "path": "skills/(tooling)/nx-generate/SKILL.md",
      "content": "# Run Nx Generator\n\nNx generators are powerful tools that scaffold projects, make automated code migrations or automate repetitive tasks in a monorepo. They ensure consistency across the codebase and reduce boilerplate work.\n\nThis skill applies when the user wants to:\n\n- Create new projects like libraries or applications\n- Scaffold features or boilerplate code\n- Run workspace-specific or custom generators\n- Do anything else that an nx generator exists for\n\n## Generator Discovery Flow\n\n### Step 1: List Available Generators\n\nUse the Nx CLI to discover available generators:\n\n- List all generators for a plugin: `npx nx list @nx/react`\n- View available plugins: `npx nx list`\n\nThis includes:\n\n- Plugin generators (e.g., `@nx/react:library`, `@nx/js:library`)\n- Local workspace generators (defined in the repo's own plugins)\n\n### Step 2: Match Generator to User Request\n\nBased on the user's request, identify which generator(s) could fulfill their needs. Consider:\n\n- What artifact type they want to create (library, application, etc.)\n- Which framework or technology stack is relevant\n- Whether they mentioned specific generator names\n\n**IMPORTANT**: When both a local workspace generator and an external plugin generator could satisfy the request, **always prefer the local workspace generator**. Local generators are customized for the specific repo's patterns and conventions.\n\nIt's possible that the user request is something that no Nx generator exists for whatsoever. In this case, you can stop using this skill and try to help the user another way. HOWEVER, the burden of proof for this is high. Before aborting, carefully consider each and every generator that's available. Look into details for any that could be related in any way before making this decision.\n\n## Pre-Execution Checklist\n\nBefore running any generator, complete these steps:\n\n### 1. Fetch Generator Schema\n\nUse the `--help` flag to understand all available options:\n\n```bash\nnpx nx g @nx/react:library --help\n```\n\nPay attention to:\n\n- Required options that must be provided\n- Optional options that may be relevant to the user's request\n- Default values that might need to be overridden\n\n### 2. Read Generator Source Code\n\nUnderstanding what the generator actually does helps you:\n\n- Know what files will be created/modified\n- Understand any side effects (updating configs, installing deps, etc.)\n- Identify options that might not be obvious from the schema\n\nTo find generator source code:\n\n- For plugin generators: Use `node -e \"console.log(require.resolve('@nx/<plugin>/generators.json'));\"` to find the generators.json, then locate the source from there\n- If that fails, read directly from `node_modules/<plugin>/generators.json`\n- For local generators: They are typically in `tools/generators/` or a local plugin directory. You can search the repo for the generator name to find it.\n\n### 2.5 Reevaluate if the generator is right\n\nOnce you have built up an understanding of what the selected generator does, reconsider: Is this the right generator to service the user request?\nIf not, it's okay to go back to the Generator Discovery Flow and select a different generator before proceeding. If you do, make sure to go through the entire pre-execution checklist once more.\n\n### 3. Understand Repo Context\n\nBefore generating, examine the target area of the codebase:\n\n- Look at similar existing artifacts (other libraries, applications, etc.)\n- Identify patterns and conventions used in the repo\n- Note naming conventions, file structures, and configuration patterns\n- Try to match these patterns when configuring the generator\n\nFor example, if similar libraries are using a specific test runner, build tool or linter, try to match that if possible.\nIf projects or other artifacts are organized with a specific naming convention, try to match it.\n\n### 4. Validate Required Options\n\nEnsure all required options have values:\n\n- Map the user's request to generator options\n- Infer values from context where possible\n- Ask the user for any critical missing information\n\n## Execution\n\nKeep in mind that you might have to prefix things with npx/pnpx/yarn if the user doesn't have nx installed globally.\nMany generators will behave differently based on where they are executed. For example, first-party nx library generators use the cwd to determine the directory that the library should be placed in. This is highly important.\n\n### Consider Dry-Run (Optional)\n\nRunning with `--dry-run` first is strongly encouraged but not mandatory. Use your judgment:\n\n- For complex generators or unfamiliar territory: do a dry-run first\n- For simple, well-understood generators: may proceed directly\n- Dry-run shows file names and created/deleted/modified markers, but not content\n- There are cases where a generator does not support dry-run (for example if it had to install an npm package) - in that case --dry-run might fail. Don't be discouraged but simply move on to running the generator for real and iterating from there.\n\n### Running the Generator\n\nExecute the generator with:\n\n```bash\nnx generate <generator-name> <options> --no-interactive\n```\n\n**CRITICAL**: Always include `--no-interactive` to prevent prompts that would hang the execution.\n\nExample:\n\n```bash\nnx generate @nx/react:library --name=my-utils --no-interactive\n```\n\n### Handling Generator Failures\n\nIf the generator fails:\n\n1. **Diagnose the error** - Read the error message carefully\n2. **Identify the cause** - Missing options, invalid values, conflicts, etc.\n3. **Attempt automatic fix** - Adjust options or resolve conflicts\n4. **Retry** - Run the generator again with corrected options\n\nCommon failure reasons:\n\n- Missing required options\n- Invalid option values\n- Conflicting with existing files\n- Missing dependencies\n- Generator doesn't support certain flag combinations\n\n## Post-Generation\n\n### 1. Modify Generated Code (If Needed)\n\nGenerators provide a starting point, but the output may need adjustment to match the user's specific requirements:\n\n- Add or modify functionality as requested\n- Adjust imports, exports, or configurations\n- Integrate with existing code patterns in the repo\n\n### 2. Format Code\n\nRun formatting on all generated/modified files:\n\n```bash\nnx format --fix\n```\n\nLanguages other than javascript/typescript might need other formatting invocations too.\n\n### 3. Run Verification\n\nVerify that the generated code works correctly. What this looks like will vary depending on the type of generator and the targets available.\nIf the generator created a new project, run its targets directly\nUse your best judgement to determine what needs to be verified.\n\nExample:\n\n```bash\nnx lint <new-project>\nnx test <new-project>\nnx build <new-project>\n```\n\n### 4. Handle Verification Failures\n\nWhen verification fails:\n\n**If scope is manageable** (a few lint errors, minor type issues):\n\n- Fix the issues\n- Re-run verification to confirm\n\n**If issues are extensive** (many errors, complex problems):\n\n- Attempt simple, obvious fixes first\n- If still failing, escalate to the user with:\n  - Description of what was generated\n  - What verification is failing\n  - What you've attempted to fix\n  - Remaining issues that need user input\n\n## Error Handling\n\n### Generator Failures\n\n- Check the error message for specific causes\n- Verify all required options are provided\n- Check for conflicts with existing files\n- Ensure the generator name and options are correct\n\n### Missing Options\n\n- Consult the generator schema for required fields\n- Infer values from context when reasonable\n- Ask the user for values that cannot be inferred\n\n## Key Principles\n\n1. **Local generators first** - Always prefer workspace/local generators over external plugin generators when both could work\n\n2. **Understand before running** - Read both the schema AND the source code to fully understand what will happen\n\n3. **No prompts** - Always use `--no-interactive` to prevent hanging\n\n4. **Generators are starting points** - Modify the output as needed to fully satisfy the user's requirements\n\n5. **Verify changes work** - Don't just generate; ensure the code builds, lints, and tests pass\n\n6. **Be proactive about fixes** - Don't just report errors; attempt to resolve them automatically when possible\n\n7. **Match repo patterns** - Study existing similar code in the repo and match its conventions",
      "metadata": {
        "hasScripts": false,
        "hasReferences": false,
        "referenceFiles": [],
        "lastModified": "2026-01-31"
      }
    },
    {
      "id": "nx-run-tasks",
      "name": "nx-run-tasks",
      "description": "Helps with running tasks in an Nx workspace. USE WHEN the user wants to execute build, test, lint, serve, or run any other tasks defined in the workspace.",
      "category": "tooling",
      "path": "skills/(tooling)/nx-run-tasks/SKILL.md",
      "content": "You can run tasks with Nx in the following way.\n\nKeep in mind that you might have to prefix things with npx/pnpx/yarn if the user doesn't have nx installed globally. Look at the package.json or lockfile to determine which package manager is in use.\n\nFor more details on any command, run it with `--help` (e.g. `nx run-many --help`, `nx affected --help`).\n\n## Understand which tasks can be run\n\nYou can check those via `nx show project <projectname> --json`, for example `nx show project myapp --json`. It contains a `targets` section which has information about targets that can be run. You can also just look at the `package.json` scripts or `project.json` targets, but you might miss out on inferred tasks by Nx plugins.\n\n## Run a single task\n\n```\nnx run <project>:<task>\n```\n\nwhere `project` is the project name defined in `package.json` or `project.json` (if present).\n\n## Run multiple tasks\n\n```\nnx run-many -t build test lint typecheck\n```\n\nYou can pass a `-p` flag to filter to specific projects, otherwise it runs on all projects. You can also use `--exclude` to exclude projects, and `--parallel` to control the number of parallel processes (default is 3).\n\nExamples:\n\n- `nx run-many -t test -p proj1 proj2` â€” test specific projects\n- `nx run-many -t test --projects=*-app --exclude=excluded-app` â€” test projects matching a pattern\n- `nx run-many -t test --projects=tag:api-*` â€” test projects by tag\n\n## Run tasks for affected projects\n\nUse `nx affected` to only run tasks on projects that have been changed and projects that depend on changed projects. This is especially useful in CI and for large workspaces.\n\n```\nnx affected -t build test lint\n```\n\nBy default it compares against the base branch. You can customize this:\n\n- `nx affected -t test --base=main --head=HEAD` â€” compare against a specific base and head\n- `nx affected -t test --files=libs/mylib/src/index.ts` â€” specify changed files directly\n\n## Useful flags\n\nThese flags work with `run`, `run-many`, and `affected`:\n\n- `--skipNxCache` â€” rerun tasks even when results are cached\n- `--verbose` â€” print additional information such as stack traces\n- `--nxBail` â€” stop execution after the first failed task\n- `--configuration=<name>` â€” use a specific configuration (e.g. `production`)",
      "metadata": {
        "hasScripts": false,
        "hasReferences": false,
        "referenceFiles": [],
        "lastModified": "2026-01-31"
      }
    },
    {
      "id": "nx-workspace",
      "name": "nx-workspace",
      "description": "Configure, explore, and optimize Nx monorepo workspaces. Use when setting up Nx, exploring workspace structure, configuring project boundaries, running tasks, analyzing affected projects, optimizing build caching, or implementing CI/CD with affected commands. Keywords - nx, monorepo, workspace, projects, targets, affected, build, lint, test.",
      "category": "tooling",
      "path": "skills/(tooling)/nx-workspace/SKILL.md",
      "content": "# Nx Workspace Management\n\n## Quick Start\n\n**Exploring workspace**: `nx show projects` and `nx show project <name> --json`  \n**Running tasks**: `nx <target> <project>` (e.g., `nx build my-app`)  \n**Affected analysis**: `nx show projects --affected` or `nx affected -t <target>`\n\n> **Note**: Prefix commands with `npx`/`pnpx`/`yarn` if nx isn't installed globally.\n\n## Core Commands\n\n### List and Explore Projects\n\n```bash\n# List all projects\nnx show projects\n\n# Filter by type, pattern, or target\nnx show projects --type app\nnx show projects --projects \"apps/*\"\nnx show projects --withTarget build\n\n# Find affected projects\nnx show projects --affected --base=main\n```\n\n### Get Project Information\n\n**Critical**: Always use `nx show project <name> --json` for full resolved configuration. Do NOT read `project.json` directly - it contains only partial configuration.\n\n```bash\n# Get full configuration\nnx show project my-app --json\n\n# Extract targets\nnx show project my-app --json | jq '.targets | keys'\n```\n\nConfiguration schemas:\n\n- Workspace: `node_modules/nx/schemas/nx-schema.json`\n- Project: `node_modules/nx/schemas/project-schema.json`\n\n### Run Tasks\n\n```bash\n# Run specific project\nnx build web --configuration=production\n\n# Run affected\nnx affected -t test --base=main\n\n# View dependency graph\nnx graph\n```\n\n## Workspace Architecture\n\n```\nworkspace/\nâ”œâ”€â”€ apps/              # Deployable applications\nâ”œâ”€â”€ libs/              # Shared libraries\nâ”‚   â”œâ”€â”€ shared/        # Shared across scopes\nâ”‚   â””â”€â”€ feature/       # Feature-specific\nâ”œâ”€â”€ nx.json            # Workspace configuration\nâ””â”€â”€ tools/             # Custom executors/generators\n```\n\n### Library Types\n\n| Type | Purpose | Example |\n|------|---------|---------|\n| **feature** | Business logic, smart components | `feature-auth` |\n| **ui** | Presentational components | `ui-buttons` |\n| **data-access** | API calls, state management | `data-access-users` |\n| **util** | Pure functions, helpers | `util-formatting` |\n\n## Detailed Resources\n\n**Configuration**: See [reference/configuration.md](reference/configuration.md) for:\n\n- nx.json templates and options\n- project.json structure\n- Module boundary rules\n- Remote caching setup\n\n**Commands**: See [reference/commands.md](reference/commands.md) for:\n\n- Complete command reference\n- Advanced filtering options\n- Common workflows\n\n**CI/CD**: See [reference/ci-cd.md](reference/ci-cd.md) for:\n\n- GitHub Actions configuration\n- GitLab CI setup\n- Jenkins, Azure Pipelines, CircleCI examples\n- Affected commands in pipelines\n\n**Best Practices**: See [reference/best-practices.md](reference/best-practices.md) for:\n\n- Do's and don'ts\n- Complete troubleshooting guide\n- Performance optimization\n- Migration guides\n\n## Common Workflows\n\n**\"What's in this workspace?\"**\n\n```bash\nnx show projects --type app  # List applications\nnx show projects --type lib  # List libraries\n```\n\n**\"How do I run project X?\"**\n\n```bash\nnx show project X --json | jq '.targets | keys'\n```\n\n**\"What changed?\"**\n\n```bash\nnx show projects --affected --base=main\n```\n\n## Quick Troubleshooting\n\n- **Targets not showing**: Use `nx show project <name> --json`, not project.json\n- **Affected not working**: Ensure git history available (`fetch-depth: 0` in CI)\n- **Cache issues**: Run `nx reset`\n\nFor detailed troubleshooting, see [reference/best-practices.md](reference/best-practices.md).",
      "metadata": {
        "hasScripts": false,
        "hasReferences": false,
        "referenceFiles": [],
        "lastModified": "2026-01-31"
      }
    },
    {
      "id": "playwright-skill",
      "name": "playwright-skill",
      "description": "Complete browser automation with Playwright. Auto-detects dev servers, writes clean test scripts to /tmp. Test pages, fill forms, take screenshots, check responsive design, validate UX, test login flows, check links, automate any browser task. Use when user wants to test websites, automate browser interactions, validate web functionality, or perform any browser-based testing.",
      "category": "web-automation",
      "path": "skills/(web-automation)/playwright-skill/SKILL.md",
      "content": "**IMPORTANT - Path Resolution:**\nThis skill can be installed in different locations (plugin system, manual installation, global, or project-specific). Before executing any commands, determine the skill directory based on where you loaded this SKILL.md file, and use that path in all commands below. Replace `$SKILL_DIR` with the actual discovered path.\n\n# Playwright Browser Automation\n\nGeneral-purpose browser automation skill. I'll write custom Playwright code for any automation task you request and execute it via the universal executor.\n\n**CRITICAL WORKFLOW - Follow these steps in order:**\n\n1. **Auto-detect dev servers** - For localhost testing, ALWAYS run server detection FIRST:\n\n   ```bash\n   cd $SKILL_DIR && node -e \"require('./lib/helpers').detectDevServers().then(servers => console.log(JSON.stringify(servers)))\"\n   ```\n\n   - If **1 server found**: Use it automatically, inform user\n   - If **multiple servers found**: Ask user which one to test\n   - If **no servers found**: Ask for URL or offer to help start dev server\n\n2. **Write scripts to /tmp** - NEVER write test files to skill directory; always use `/tmp/playwright-test-*.js`\n\n3. **Use visible browser by default** - Always use `headless: false` unless user specifically requests headless mode\n\n4. **Parameterize URLs** - Always make URLs configurable via environment variable or constant at top of script\n\n## How It Works\n\n1. You describe what you want to test/automate\n2. I auto-detect running dev servers (or ask for URL if testing external site)\n3. I write custom Playwright code in `/tmp/playwright-test-*.js` (won't clutter your project)\n4. I execute it via: `cd $SKILL_DIR && node run.js /tmp/playwright-test-*.js`\n5. Results displayed in real-time, browser window visible for debugging\n6. Test files auto-cleaned from /tmp by your OS\n\n## Setup (First Time)\n\n```bash\ncd $SKILL_DIR\nnpm run setup\n```\n\nThis installs Playwright and Chromium browser. Only needed once.\n\n## Execution Pattern\n\n**Step 1: Detect dev servers (for localhost testing)**\n\n```bash\ncd $SKILL_DIR && node -e \"require('./lib/helpers').detectDevServers().then(s => console.log(JSON.stringify(s)))\"\n```\n\n**Step 2: Write test script to /tmp with URL parameter**\n\n```javascript\n// /tmp/playwright-test-page.js\nconst { chromium } = require('playwright');\n\n// Parameterized URL (detected or user-provided)\nconst TARGET_URL = 'http://localhost:3001'; // <-- Auto-detected or from user\n\n(async () => {\n  const browser = await chromium.launch({ headless: false });\n  const page = await browser.newPage();\n\n  await page.goto(TARGET_URL);\n  console.log('Page loaded:', await page.title());\n\n  await page.screenshot({ path: '/tmp/screenshot.png', fullPage: true });\n  console.log('ðŸ“¸ Screenshot saved to /tmp/screenshot.png');\n\n  await browser.close();\n})();\n```\n\n**Step 3: Execute from skill directory**\n\n```bash\ncd $SKILL_DIR && node run.js /tmp/playwright-test-page.js\n```\n\n## Common Patterns\n\n### Test a Page (Multiple Viewports)\n\n```javascript\n// /tmp/playwright-test-responsive.js\nconst { chromium } = require('playwright');\n\nconst TARGET_URL = 'http://localhost:3001'; // Auto-detected\n\n(async () => {\n  const browser = await chromium.launch({ headless: false, slowMo: 100 });\n  const page = await browser.newPage();\n\n  // Desktop test\n  await page.setViewportSize({ width: 1920, height: 1080 });\n  await page.goto(TARGET_URL);\n  console.log('Desktop - Title:', await page.title());\n  await page.screenshot({ path: '/tmp/desktop.png', fullPage: true });\n\n  // Mobile test\n  await page.setViewportSize({ width: 375, height: 667 });\n  await page.screenshot({ path: '/tmp/mobile.png', fullPage: true });\n\n  await browser.close();\n})();\n```\n\n### Test Login Flow\n\n```javascript\n// /tmp/playwright-test-login.js\nconst { chromium } = require('playwright');\n\nconst TARGET_URL = 'http://localhost:3001'; // Auto-detected\n\n(async () => {\n  const browser = await chromium.launch({ headless: false });\n  const page = await browser.newPage();\n\n  await page.goto(`${TARGET_URL}/login`);\n\n  await page.fill('input[name=\"email\"]', 'test@example.com');\n  await page.fill('input[name=\"password\"]', 'password123');\n  await page.click('button[type=\"submit\"]');\n\n  // Wait for redirect\n  await page.waitForURL('**/dashboard');\n  console.log('âœ… Login successful, redirected to dashboard');\n\n  await browser.close();\n})();\n```\n\n### Fill and Submit Form\n\n```javascript\n// /tmp/playwright-test-form.js\nconst { chromium } = require('playwright');\n\nconst TARGET_URL = 'http://localhost:3001'; // Auto-detected\n\n(async () => {\n  const browser = await chromium.launch({ headless: false, slowMo: 50 });\n  const page = await browser.newPage();\n\n  await page.goto(`${TARGET_URL}/contact`);\n\n  await page.fill('input[name=\"name\"]', 'John Doe');\n  await page.fill('input[name=\"email\"]', 'john@example.com');\n  await page.fill('textarea[name=\"message\"]', 'Test message');\n  await page.click('button[type=\"submit\"]');\n\n  // Verify submission\n  await page.waitForSelector('.success-message');\n  console.log('âœ… Form submitted successfully');\n\n  await browser.close();\n})();\n```\n\n### Check for Broken Links\n\n```javascript\nconst { chromium } = require('playwright');\n\n(async () => {\n  const browser = await chromium.launch({ headless: false });\n  const page = await browser.newPage();\n\n  await page.goto('http://localhost:3000');\n\n  const links = await page.locator('a[href^=\"http\"]').all();\n  const results = { working: 0, broken: [] };\n\n  for (const link of links) {\n    const href = await link.getAttribute('href');\n    try {\n      const response = await page.request.head(href);\n      if (response.ok()) {\n        results.working++;\n      } else {\n        results.broken.push({ url: href, status: response.status() });\n      }\n    } catch (e) {\n      results.broken.push({ url: href, error: e.message });\n    }\n  }\n\n  console.log(`âœ… Working links: ${results.working}`);\n  console.log(`âŒ Broken links:`, results.broken);\n\n  await browser.close();\n})();\n```\n\n### Take Screenshot with Error Handling\n\n```javascript\nconst { chromium } = require('playwright');\n\n(async () => {\n  const browser = await chromium.launch({ headless: false });\n  const page = await browser.newPage();\n\n  try {\n    await page.goto('http://localhost:3000', {\n      waitUntil: 'networkidle',\n      timeout: 10000,\n    });\n\n    await page.screenshot({\n      path: '/tmp/screenshot.png',\n      fullPage: true,\n    });\n\n    console.log('ðŸ“¸ Screenshot saved to /tmp/screenshot.png');\n  } catch (error) {\n    console.error('âŒ Error:', error.message);\n  } finally {\n    await browser.close();\n  }\n})();\n```\n\n### Test Responsive Design\n\n```javascript\n// /tmp/playwright-test-responsive-full.js\nconst { chromium } = require('playwright');\n\nconst TARGET_URL = 'http://localhost:3001'; // Auto-detected\n\n(async () => {\n  const browser = await chromium.launch({ headless: false });\n  const page = await browser.newPage();\n\n  const viewports = [\n    { name: 'Desktop', width: 1920, height: 1080 },\n    { name: 'Tablet', width: 768, height: 1024 },\n    { name: 'Mobile', width: 375, height: 667 },\n  ];\n\n  for (const viewport of viewports) {\n    console.log(\n      `Testing ${viewport.name} (${viewport.width}x${viewport.height})`,\n    );\n\n    await page.setViewportSize({\n      width: viewport.width,\n      height: viewport.height,\n    });\n\n    await page.goto(TARGET_URL);\n    await page.waitForTimeout(1000);\n\n    await page.screenshot({\n      path: `/tmp/${viewport.name.toLowerCase()}.png`,\n      fullPage: true,\n    });\n  }\n\n  console.log('âœ… All viewports tested');\n  await browser.close();\n})();\n```\n\n## Inline Execution (Simple Tasks)\n\nFor quick one-off tasks, you can execute code inline without creating files:\n\n```bash\n# Take a quick screenshot\ncd $SKILL_DIR && node run.js \"\nconst browser = await chromium.launch({ headless: false });\nconst page = await browser.newPage();\nawait page.goto('http://localhost:3001');\nawait page.screenshot({ path: '/tmp/quick-screenshot.png', fullPage: true });\nconsole.log('Screenshot saved');\nawait browser.close();\n\"\n```\n\n**When to use inline vs files:**\n\n- **Inline**: Quick one-off tasks (screenshot, check if element exists, get page title)\n- **Files**: Complex tests, responsive design checks, anything user might want to re-run\n\n## Available Helpers\n\nOptional utility functions in `lib/helpers.js`:\n\n```javascript\nconst helpers = require('./lib/helpers');\n\n// Detect running dev servers (CRITICAL - use this first!)\nconst servers = await helpers.detectDevServers();\nconsole.log('Found servers:', servers);\n\n// Safe click with retry\nawait helpers.safeClick(page, 'button.submit', { retries: 3 });\n\n// Safe type with clear\nawait helpers.safeType(page, '#username', 'testuser');\n\n// Take timestamped screenshot\nawait helpers.takeScreenshot(page, 'test-result');\n\n// Handle cookie banners\nawait helpers.handleCookieBanner(page);\n\n// Extract table data\nconst data = await helpers.extractTableData(page, 'table.results');\n```\n\nSee `lib/helpers.js` for full list.\n\n## Custom HTTP Headers\n\nConfigure custom headers for all HTTP requests via environment variables. Useful for:\n\n- Identifying automated traffic to your backend\n- Getting LLM-optimized responses (e.g., plain text errors instead of styled HTML)\n- Adding authentication tokens globally\n\n### Configuration\n\n**Single header (common case):**\n\n```bash\nPW_HEADER_NAME=X-Automated-By PW_HEADER_VALUE=playwright-skill \\\n  cd $SKILL_DIR && node run.js /tmp/my-script.js\n```\n\n**Multiple headers (JSON format):**\n\n```bash\nPW_EXTRA_HEADERS='{\"X-Automated-By\":\"playwright-skill\",\"X-Debug\":\"true\"}' \\\n  cd $SKILL_DIR && node run.js /tmp/my-script.js\n```\n\n### How It Works\n\nHeaders are automatically applied when using `helpers.createContext()`:\n\n```javascript\nconst context = await helpers.createContext(browser);\nconst page = await context.newPage();\n// All requests from this page include your custom headers\n```\n\nFor scripts using raw Playwright API, use the injected `getContextOptionsWithHeaders()`:\n\n```javascript\nconst context = await browser.newContext(\n  getContextOptionsWithHeaders({ viewport: { width: 1920, height: 1080 } }),\n);\n```\n\n## Advanced Usage\n\nFor comprehensive Playwright API documentation, see [API_REFERENCE.md](API_REFERENCE.md):\n\n- Selectors & Locators best practices\n- Network interception & API mocking\n- Authentication & session management\n- Visual regression testing\n- Mobile device emulation\n- Performance testing\n- Debugging techniques\n- CI/CD integration\n\n## Tips\n\n- **CRITICAL: Detect servers FIRST** - Always run `detectDevServers()` before writing test code for localhost testing\n- **Custom headers** - Use `PW_HEADER_NAME`/`PW_HEADER_VALUE` env vars to identify automated traffic to your backend\n- **Use /tmp for test files** - Write to `/tmp/playwright-test-*.js`, never to skill directory or user's project\n- **Parameterize URLs** - Put detected/provided URL in a `TARGET_URL` constant at the top of every script\n- **DEFAULT: Visible browser** - Always use `headless: false` unless user explicitly asks for headless mode\n- **Headless mode** - Only use `headless: true` when user specifically requests \"headless\" or \"background\" execution\n- **Slow down:** Use `slowMo: 100` to make actions visible and easier to follow\n- **Wait strategies:** Use `waitForURL`, `waitForSelector`, `waitForLoadState` instead of fixed timeouts\n- **Error handling:** Always use try-catch for robust automation\n- **Console output:** Use `console.log()` to track progress and show what's happening\n\n## Troubleshooting\n\n**Playwright not installed:**\n\n```bash\ncd $SKILL_DIR && npm run setup\n```\n\n**Module not found:**\nEnsure running from skill directory via `run.js` wrapper\n\n**Browser doesn't open:**\nCheck `headless: false` and ensure display available\n\n**Element not found:**\nAdd wait: `await page.waitForSelector('.element', { timeout: 10000 })`\n\n## Example Usage\n\n```\nUser: \"Test if the marketing page looks good\"\n\nClaude: I'll test the marketing page across multiple viewports. Let me first detect running servers...\n[Runs: detectDevServers()]\n[Output: Found server on port 3001]\nI found your dev server running on http://localhost:3001\n\n[Writes custom automation script to /tmp/playwright-test-marketing.js with URL parameterized]\n[Runs: cd $SKILL_DIR && node run.js /tmp/playwright-test-marketing.js]\n[Shows results with screenshots from /tmp/]\n```\n\n```\nUser: \"Check if login redirects correctly\"\n\nClaude: I'll test the login flow. First, let me check for running servers...\n[Runs: detectDevServers()]\n[Output: Found servers on ports 3000 and 3001]\nI found 2 dev servers. Which one should I test?\n- http://localhost:3000\n- http://localhost:3001\n\nUser: \"Use 3001\"\n\n[Writes login automation to /tmp/playwright-test-login.js]\n[Runs: cd $SKILL_DIR && node run.js /tmp/playwright-test-login.js]\n[Reports: âœ… Login successful, redirected to /dashboard]\n```\n\n## Notes\n\n- Each automation is custom-written for your specific request\n- Not limited to pre-built scripts - any browser task possible\n- Auto-detects running dev servers to eliminate hardcoded URLs\n- Test scripts written to `/tmp` for automatic cleanup (no clutter)\n- Code executes reliably with proper module resolution via `run.js`\n- Progressive disclosure - API_REFERENCE.md loaded only when advanced features needed",
      "metadata": {
        "hasScripts": false,
        "hasReferences": false,
        "referenceFiles": [],
        "lastModified": "2026-01-31"
      }
    },
    {
      "id": "run-nx-generator",
      "name": "run-nx-generator",
      "description": "Run Nx generators with prioritization for workspace-plugin generators. Use this when generating code, scaffolding new features, or automating repetitive tasks in the monorepo.",
      "category": "tooling",
      "path": "skills/(tooling)/run-nx-generator/SKILL.md",
      "content": "# Run Nx Generator\n\nThis skill helps you execute Nx generators efficiently, with special focus on workspace-plugin generators from your internal tooling.\n\n## Generator Priority List\n\nUse the `mcp__nx-mcp__nx_generator_schema` tool to get more information about how to use the generator\n\nChoose which generators to run in this priority order:\n\n### ðŸ”¥ Workspace-Plugin Generators (High Priority)\n\nThese are your custom internal tools in `tools/workspace-plugin/`\n\n### ðŸ“¦ Core Nx Generators (Standard)\n\nOnly use these if workspace-plugin generators don't fit:\n\n- `nx generate @nx/devkit:...` - DevKit utilities\n- `nx generate @nx/node:...` - Node.js libraries\n- `nx generate @nx/react:...` - React components and apps\n- Framework-specific generators\n\n## How to Run Generators\n\n1. **List available generators**:\n\n2. **Get generator schema** (to see available options):\n   Use the `mcp__nx-mcp__nx_generator_schema` tool to get more information about how to use the generator\n\n3. **Run the generator**:\n\n   ```bash\n   nx generate [generator-path] [options]\n   ```\n\n4. **Verify the changes**:\n   - Review generated files\n   - Run tests: `nx affected -t test`\n   - Format code: `npx prettier --write [files]`\n\n## Best Practices\n\n- âœ… Always check workspace-plugin first - it has your custom solutions\n- âœ… Use `--dry-run` flag to preview changes before applying\n- âœ… Format generated code immediately with Prettier\n- âœ… Test affected projects after generation\n- âœ… Commit generator changes separately from manual edits\n\n## Examples\n\n### Bumping Maven Version\n\nWhen updating the Maven plugin version, use the workspace-plugin generator:\n\n```bash\nnx generate @nx/workspace-plugin:bump-maven-version \\\n  --newVersion 0.0.10 \\\n  --nxVersion 22.1.0-beta.7\n```\n\nThis automates all the version bumping instead of manual file edits.\n\n### Creating a New Plugin\n\nFor creating a new create-nodes plugin:\n\n```bash\nnx generate @nx/workspace-plugin:create-nodes-plugin \\\n  --name my-custom-plugin\n```\n\n## When to Use This Skill\n\nUse this skill when you need to:\n\n- Generate new code or projects\n- Scaffold new features or libraries\n- Automate repetitive setup tasks\n- Update internal tools and configurations\n- Create migrations or version updates",
      "metadata": {
        "hasScripts": false,
        "hasReferences": false,
        "referenceFiles": [],
        "lastModified": "2026-01-31"
      }
    },
    {
      "id": "skill-creator",
      "name": "skill-creator",
      "description": "Guide for creating effective AI agent skills. Use when users want to create a new skill (or update an existing skill) that extends an AI agent's capabilities with specialized knowledge, workflows, or tool integrations. Works with any agent that supports the SKILL.md format (Claude Code, Cursor, Roo, Cline, Windsurf, etc.). Triggers on \"create skill\", \"new skill\", \"package knowledge\", \"skill for\".",
      "category": "creation",
      "path": "skills/(creation)/skill-creator/SKILL.md",
      "content": "# Skill Creator\n\nThis skill provides guidance for creating effective, agent-agnostic skills.\n\n## About Skills\n\nSkills are modular, self-contained packages that extend AI agent capabilities by providing specialized knowledge, workflows, and tools. Think of them as \"onboarding guides\" for specific domains or tasksâ€”they transform a general-purpose agent into a specialized agent equipped with procedural knowledge.\n\n### What Skills Provide\n\n1. **Specialized workflows** - Multi-step procedures for specific domains\n2. **Tool integrations** - Instructions for working with specific file formats or APIs\n3. **Domain expertise** - Company-specific knowledge, schemas, business logic\n4. **Bundled resources** - Scripts, references, and assets for complex and repetitive tasks\n\n## Core Principles\n\n### Concise is Key\n\nThe context window is a public good. Skills share context with everything else the agent needs.\n\n**Default assumption: The agent is already very smart.** Only add context it doesn't already have. Challenge each piece of information: \"Does the agent really need this?\" and \"Does this paragraph justify its token cost?\"\n\nPrefer concise examples over verbose explanations.\n\n### Anatomy of a Skill\n\nEvery skill consists of a required SKILL.md file and optional bundled resources:\n\n```\nskill-name/\nâ”œâ”€â”€ SKILL.md (required)\nâ”‚   â”œâ”€â”€ YAML frontmatter metadata (required)\nâ”‚   â”‚   â”œâ”€â”€ name: (required)\nâ”‚   â”‚   â””â”€â”€ description: (required)\nâ”‚   â””â”€â”€ Markdown instructions (required)\nâ””â”€â”€ Bundled Resources (optional)\n    â”œâ”€â”€ scripts/          - Executable code (Python/Bash/etc.)\n    â”œâ”€â”€ references/       - Documentation loaded into context as needed\n    â””â”€â”€ assets/           - Files used in output (templates, icons, fonts, etc.)\n```\n\n#### SKILL.md (required)\n\nEvery SKILL.md consists of:\n\n- **Frontmatter** (YAML): Contains `name` and `description` fields. These are the only fields read to determine when the skill gets usedâ€”be clear and comprehensive.\n- **Body** (Markdown): Instructions and guidance for using the skill. Only loaded AFTER the skill triggers.\n\n#### Bundled Resources (optional)\n\n##### Scripts (`scripts/`)\n\nExecutable code for tasks that require deterministic reliability or are repeatedly rewritten.\n\n- **When to include**: When the same code is being rewritten repeatedly\n- **Example**: `scripts/rotate_pdf.py` for PDF rotation tasks\n- **Benefits**: Token efficient, deterministic\n\n##### References (`references/`)\n\nDocumentation and reference material loaded as needed into context.\n\n- **When to include**: For documentation the agent should reference while working\n- **Examples**: `references/schema.md` for database schemas, `references/api_docs.md` for API specifications\n- **Benefits**: Keeps SKILL.md lean, loaded only when needed\n\n##### Assets (`assets/`)\n\nFiles not intended to be loaded into context, but used within output.\n\n- **When to include**: When the skill needs files for final output\n- **Examples**: `assets/logo.png` for brand assets, `assets/template.html` for HTML boilerplate\n\n### Progressive Disclosure\n\nSkills use a three-level loading system:\n\n1. **Metadata (name + description)** - Always in context (~100 words)\n2. **SKILL.md body** - When skill triggers (<5k words)\n3. **Bundled resources** - As needed (unlimited)\n\nKeep SKILL.md body under 500 lines. Split content into separate files when approaching this limit.\n\n## Skill Creation Process\n\n### Step 1: Understand the Skill\n\nClarify with concrete examples:\n\n- \"What functionality should this skill support?\"\n- \"Can you give examples of how this skill would be used?\"\n- \"What would trigger this skill?\"\n\n### Step 2: Plan Reusable Contents\n\nAnalyze each example:\n\n1. Consider how to execute from scratch\n2. Identify helpful scripts, references, and assets\n\n### Step 3: Create the Skill\n\nCreate the skill directory:\n\n```\nskill-name/\nâ”œâ”€â”€ SKILL.md\nâ”œâ”€â”€ scripts/     (if needed)\nâ”œâ”€â”€ references/  (if needed)\nâ””â”€â”€ assets/      (if needed)\n```\n\n### Step 4: Write SKILL.md\n\n#### Frontmatter\n\n```yaml\n---\nname: skill-name\ndescription: What the skill does and when to use it. Include specific triggers and contexts. Max 1024 characters.\n---\n```\n\n**Description guidelines:**\n- Include both what the skill does AND when to use it\n- Include trigger phrases\n- Max 1024 characters, no XML tags\n- Write in third person\n\n#### Body\n\nWrite instructions for using the skill. Include:\n- Quick start guide\n- Step-by-step workflow\n- Links to reference files when needed\n\n### Step 5: Test and Iterate\n\n1. Use the skill on real tasks\n2. Notice struggles or inefficiencies\n3. Update SKILL.md or resources accordingly\n4. Test again\n\n## Quality Checklist\n\nBefore finalizing:\n\n- [ ] Description is specific about when to use (max 1024 chars)\n- [ ] Folder name uses kebab-case\n- [ ] Instructions are actionable and unambiguous\n- [ ] Scope is focused (one responsibility)\n- [ ] SKILL.md body < 500 lines\n- [ ] References are one level deep from SKILL.md\n\n## Output Messages\n\nWhen creating a skill, inform the user:\n\n```\nâœ… Skill created successfully!\n\nðŸ“ Location: .agent/skills/[name]/SKILL.md\nðŸŽ¯ Purpose: [brief description]\nðŸ”§ How to test: [example prompt that should trigger the skill]\n\nðŸ’¡ Tip: The agent will use this skill automatically when it detects [context].\n```",
      "metadata": {
        "hasScripts": false,
        "hasReferences": false,
        "referenceFiles": [],
        "lastModified": "2026-01-31"
      }
    },
    {
      "id": "spec-driven-dev",
      "name": "spec-driven-dev",
      "description": "Feature planning with 4 phases - Specify requirements, Design architecture, break into granular Tasks, Implement and Validate. Creates atomic tasks that agents can implement without errors. Triggers on \"plan feature\", \"design\", \"new feature\", \"implement feature\", \"create spec\".",
      "category": "development",
      "path": "skills/(development)/spec-driven-dev/SKILL.md",
      "content": "# Spec-Driven Development\n\nPlan and implement features with precision. Granular tasks. Clear dependencies. Right tools.\n\n```\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ SPECIFY  â”‚ â†’ â”‚  DESIGN  â”‚ â†’ â”‚  TASKS  â”‚ â†’ â”‚ IMPLEMENT+VALIDATEâ”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```\n\n## Phase Selection\n\n| User wants to... | Load reference |\n|------------------|----------------|\n| Define what to build | [specify.md](references/specify.md) |\n| Design architecture | [design.md](references/design.md) |\n| Break into tasks | [tasks.md](references/tasks.md) |\n| Implement a task | [implement.md](references/implement.md) |\n| Verify it works | [validate.md](references/validate.md) |\n\n## Commands\n\n| Command | Action |\n|---------|--------|\n| `specify [feature]` | Define requirements |\n| `design [feature]` | Design architecture |\n| `tasks [feature]` | Create task breakdown |\n| `implement T1` | Implement task |\n| `validate` | Verify implementation |\n\n## Output\n\n```\n.specs/[feature-slug]/\nâ”œâ”€â”€ spec.md\nâ”œâ”€â”€ design.md\nâ””â”€â”€ tasks.md\n```",
      "metadata": {
        "hasScripts": false,
        "hasReferences": true,
        "referenceFiles": [
          "design.md",
          "implement.md",
          "specify.md",
          "tasks.md",
          "validate.md"
        ],
        "lastModified": "2026-01-31"
      }
    },
    {
      "id": "subagent-creator",
      "name": "subagent-creator",
      "description": "Guide for creating AI subagents with isolated context for complex multi-step workflows. Use when users want to create a subagent, specialized agent, verifier, debugger, or orchestrator that requires isolated context and deep specialization. Works with any agent that supports subagent delegation. Triggers on \"create subagent\", \"new agent\", \"specialized assistant\", \"create verifier\".",
      "category": "creation",
      "path": "skills/(creation)/subagent-creator/SKILL.md",
      "content": "# Subagent Creator\n\nThis skill provides guidance for creating effective, agent-agnostic subagents.\n\n## What are Subagents?\n\nSubagents are specialized assistants that an AI agent can delegate tasks to. Characteristics:\n\n- **Isolated context**: Each subagent has its own context window\n- **Parallel execution**: Multiple subagents can run simultaneously\n- **Specialization**: Configured with specific prompts and expertise\n- **Reusable**: Defined once, used in multiple contexts\n\n### When to Use Subagents vs Skills\n\n```\nIs the task complex with multiple steps?\nâ”œâ”€ YES â†’ Does it require isolated context?\nâ”‚         â”œâ”€ YES â†’ Use SUBAGENT\nâ”‚         â””â”€ NO â†’ Use SKILL\nâ”‚\nâ””â”€ NO â†’ Use SKILL\n```\n\n**Use Subagents for:**\n- Complex workflows requiring isolated context\n- Long-running tasks that benefit from specialization\n- Verification and auditing (independent perspective)\n- Parallel workstreams\n\n**Use Skills for:**\n- Quick, one-off actions\n- Domain knowledge without context isolation\n- Reusable procedures that don't need isolation\n\n## Subagent Structure\n\nA subagent is typically a markdown file with frontmatter metadata:\n\n```markdown\n---\nname: agent-name\ndescription: Description of when to use this subagent.\nmodel: inherit  # or fast, or specific model ID\nreadonly: false  # true to restrict write permissions\n---\n\nYou are an [expert in X].\n\nWhen invoked:\n1. [Step 1]\n2. [Step 2]\n3. [Step 3]\n\n[Detailed instructions about expected behavior]\n\nReport [type of expected result]:\n- [Output format]\n- [Metrics or specific information]\n```\n\n## Subagent Creation Process\n\n### 1. Define the Purpose\n\n- What specific responsibility does the subagent have?\n- Why does it need isolated context?\n- Does it involve multiple complex steps?\n- Does it require deep specialization?\n\n### 2. Configure the Metadata\n\n#### name (required)\nUnique identifier. Use kebab-case.\n\n```yaml\nname: security-auditor\n```\n\n#### description (critical)\nCRITICAL for automatic delegation. Explains when to use this subagent.\n\n**Good descriptions:**\n- \"Security specialist. Use when implementing auth, payments, or handling sensitive data.\"\n- \"Debugging specialist for errors and test failures. Use when encountering issues.\"\n- \"Validates completed work. Use after tasks are marked done.\"\n\n**Phrases that encourage automatic delegation:**\n- \"Use proactively when...\"\n- \"Always use for...\"\n- \"Automatically delegate when...\"\n\n#### model (optional)\n```yaml\nmodel: inherit  # Uses same model as parent (default)\nmodel: fast     # Uses fast model for quick tasks\n```\n\n#### readonly (optional)\n```yaml\nreadonly: true  # Restricts write permissions\n```\n\n### 3. Write the Subagent Prompt\n\nDefine:\n1. **Identity**: \"You are an [expert]...\"\n2. **When invoked**: Context of use\n3. **Process**: Specific steps to follow\n4. **Expected output**: Format and content\n\n**Template:**\n\n```markdown\nYou are an [expert in X] specialized in [Y].\n\nWhen invoked:\n1. [First action]\n2. [Second action]\n3. [Third action]\n\n[Detailed instructions about approach]\n\nReport [type of result]:\n- [Specific format]\n- [Information to include]\n- [Metrics or criteria]\n\n[Philosophy or principles to follow]\n```\n\n## Common Subagent Patterns\n\n### 1. Verification Agent\n\n**Purpose**: Independently validates that completed work actually works.\n\n```markdown\n---\nname: verifier\ndescription: Validates completed work. Use after tasks are marked done.\nmodel: fast\n---\n\nYou are a skeptical validator.\n\nWhen invoked:\n1. Identify what was declared as complete\n2. Verify the implementation exists and is functional\n3. Execute tests or relevant verification steps\n4. Look for edge cases that may have been missed\n\nBe thorough. Report:\n- What was verified and passed\n- What is incomplete or broken\n- Specific issues to address\n```\n\n### 2. Debugger\n\n**Purpose**: Expert in root cause analysis.\n\n```markdown\n---\nname: debugger\ndescription: Debugging specialist. Use when encountering errors or test failures.\n---\n\nYou are a debugging expert.\n\nWhen invoked:\n1. Capture the error message and stack trace\n2. Identify reproduction steps\n3. Isolate the failure location\n4. Implement minimal fix\n5. Verify the solution works\n\nFor each issue, provide:\n- Root cause explanation\n- Evidence supporting the diagnosis\n- Specific code fix\n- Testing approach\n```\n\n### 3. Security Auditor\n\n**Purpose**: Security expert auditing code.\n\n```markdown\n---\nname: security-auditor\ndescription: Security specialist. Use for auth, payments, or sensitive data.\n---\n\nYou are a security expert.\n\nWhen invoked:\n1. Identify security-sensitive code paths\n2. Check for common vulnerabilities\n3. Confirm secrets are not hardcoded\n4. Review input validation\n\nReport findings by severity:\n- **Critical** (must fix before deploy)\n- **High** (fix soon)\n- **Medium** (address when possible)\n- **Low** (suggestions)\n```\n\n### 4. Code Reviewer\n\n**Purpose**: Code review with focus on quality.\n\n```markdown\n---\nname: code-reviewer\ndescription: Code review specialist. Use when changes are ready for review.\n---\n\nYou are a code review expert.\n\nWhen invoked:\n1. Analyze the code changes\n2. Check readability, performance, patterns, error handling\n3. Identify code smells and potential bugs\n4. Suggest specific improvements\n\nReport:\n**âœ… Approved / âš ï¸ Approved with caveats / âŒ Changes needed**\n\n**Issues Found:**\n- **[Severity]** [Location]: [Issue]\n  - Suggestion: [How to fix]\n```\n\n## Best Practices\n\n### âœ… DO\n\n- **Write focused subagents**: One clear responsibility\n- **Invest in the description**: Determines when to delegate\n- **Keep prompts concise**: Direct and specific\n- **Share with team**: Version control subagent definitions\n- **Test the description**: Check correct subagent is triggered\n\n### âŒ AVOID\n\n- **Vague descriptions**: \"Use for general tasks\" gives no signal\n- **Prompts too long**: 2000 words don't make it smarter\n- **Too many subagents**: Start with 2-3 focused ones\n\n## Quality Checklist\n\nBefore finalizing:\n\n- [ ] Description is specific about when to delegate\n- [ ] Name uses kebab-case\n- [ ] One clear responsibility (not generic)\n- [ ] Prompt is concise but complete\n- [ ] Instructions are actionable\n- [ ] Output format is well defined\n- [ ] Model configuration appropriate\n\n## Output Messages\n\nWhen creating a subagent:\n\n```\nâœ… Subagent created successfully!\n\nðŸ“ Location: .agent/subagents/[name].md\nðŸŽ¯ Purpose: [brief description]\nðŸ”§ How to invoke:\n   - Automatic: Agent delegates when it detects [context]\n   - Explicit: /[name] [instruction]\n\nðŸ’¡ Tip: Include keywords like \"use proactively\" to encourage delegation.\n```",
      "metadata": {
        "hasScripts": false,
        "hasReferences": false,
        "referenceFiles": [],
        "lastModified": "2026-01-31"
      }
    }
  ],
  "categories": [
    {
      "id": "cloud",
      "name": "Cloud & Infrastructure",
      "description": "Skills for cloud management, AWS, and DevOps",
      "priority": 1
    },
    {
      "id": "creation",
      "name": "Skill & Agent Creation",
      "description": "Skills for creating new skills and subagents",
      "priority": 2
    },
    {
      "id": "development",
      "name": "Development",
      "description": "Skills for software development workflows",
      "priority": 3
    },
    {
      "id": "tooling",
      "name": "Tooling",
      "description": "Skills for tooling and utilities",
      "priority": 4
    },
    {
      "id": "web-automation",
      "name": "Web Automation",
      "description": "Skills for browser automation and web testing",
      "priority": 5
    }
  ],
  "stats": {
    "totalSkills": 14,
    "totalCategories": 5
  }
}